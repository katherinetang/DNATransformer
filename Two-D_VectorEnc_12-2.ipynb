{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83acf3e0-07d8-4d89-b480-db1c78c056ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of combined dataset after filtering: 1294216\n",
      "Training set size: 1292921\n",
      "Test set size: 1295\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load all datasets\n",
    "with open('database1_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database1_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database2_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database2_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database3_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database3_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database4_1structsonly_34ave.pkl', 'rb') as file:\n",
    "    database4_1structsonly_34ave = pickle.load(file)\n",
    "\n",
    "with open('database5_1structsonly_50ave.pkl', 'rb') as file:\n",
    "    database5_1structsonly_50ave = pickle.load(file)\n",
    "\n",
    "with open('database6_allstructs_50ave.pkl', 'rb') as file:\n",
    "    database6_allstructs_50ave = pickle.load(file)\n",
    "\n",
    "with open('database7_allstructs_50ave.pkl', 'rb') as file:\n",
    "    database7_allstructs_50ave = pickle.load(file)\n",
    "\n",
    "with open('database9_allstructs_40ave.pkl', 'rb') as file:\n",
    "    database9_allstructs_40ave = pickle.load(file)\n",
    "\n",
    "with open('database10_allstructs_40ave.pkl', 'rb') as file:\n",
    "    database10_allstructs_40ave = pickle.load(file)\n",
    "\n",
    "with open('database11_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database11_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database13_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database13_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database14_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database14_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database15_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database15_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database16_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database16_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database17_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database17_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database18_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database18_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database19_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database19_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database20_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database20_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database21_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database21_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database22_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database22_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database23_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database23_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database24_allstructs_45only.pkl', 'rb') as file:\n",
    "    database24_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database25_allstructs_45only.pkl', 'rb') as file:\n",
    "    database25_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database26_allstructs_45only.pkl', 'rb') as file:\n",
    "    database26_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database27_allstructs_45only.pkl', 'rb') as file:\n",
    "    database27_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database28_allstructs_45only.pkl', 'rb') as file:\n",
    "    database28_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database29_allstructs_45only.pkl', 'rb') as file:\n",
    "    database29_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database30_allstructs_45only.pkl', 'rb') as file:\n",
    "    database30_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database31_allstructs_45only.pkl', 'rb') as file:\n",
    "    database31_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database32_allstructs_45only.pkl', 'rb') as file:\n",
    "    database32_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database33_allstructs_45only.pkl', 'rb') as file:\n",
    "    database33_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database34_allstructs_45only.pkl', 'rb') as file:\n",
    "    database34_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database35_allstructs_45only.pkl', 'rb') as file:\n",
    "    database35_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database36_allstructs_45only.pkl', 'rb') as file:\n",
    "    database36_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database37_allstructs_45only.pkl', 'rb') as file:\n",
    "    database37_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database38_allstructs_45only.pkl', 'rb') as file:\n",
    "    database38_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database39_allstructs_45only.pkl', 'rb') as file:\n",
    "    database39_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database40_allstructs_45only.pkl', 'rb') as file:\n",
    "    database40_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database41_allstructs_45only.pkl', 'rb') as file:\n",
    "    database41_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database42_allstructs_45only.pkl', 'rb') as file:\n",
    "    database42_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database43_allstructs_45only.pkl', 'rb') as file:\n",
    "    database43_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database44_allstructs_45only.pkl', 'rb') as file:\n",
    "    database44_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database45_allstructs_45only.pkl', 'rb') as file:\n",
    "    database45_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database46_allstructs_45only.pkl', 'rb') as file:\n",
    "    database46_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database47_allstructs_45only.pkl', 'rb') as file:\n",
    "    database47_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database48_allstructs_45only.pkl', 'rb') as file:\n",
    "    database48_allstructs_45only = pickle.load(file)\n",
    "    \n",
    "\n",
    "# Combine datasets into a single dictionary\n",
    "combined_data = {}\n",
    "datasets = [\n",
    "    database1_allstructs_34ave, database2_allstructs_34ave, database3_allstructs_34ave,\n",
    "    database4_1structsonly_34ave, database5_1structsonly_50ave, database6_allstructs_50ave,\n",
    "    database7_allstructs_50ave, database9_allstructs_40ave, database10_allstructs_40ave, \n",
    "    database11_allstructs_45ave, database13_allstructs_45ave, database14_allstructs_45ave,\n",
    "    database15_allstructs_45ave, database16_allstructs_45ave, database17_allstructs_45ave,\n",
    "    database18_allstructs_45ave, database19_allstructs_45ave, database20_allstructs_45ave,\n",
    "    database21_allstructs_45ave, database22_allstructs_45ave, database23_allstructs_45ave,\n",
    "    database24_allstructs_45only, database25_allstructs_45only, database26_allstructs_45only,\n",
    "    database27_allstructs_45only, database28_allstructs_45only, database29_allstructs_45only,\n",
    "    database30_allstructs_45only, database31_allstructs_45only, database32_allstructs_45only,\n",
    "    database33_allstructs_45only, database34_allstructs_45only, database35_allstructs_45only,\n",
    "    database36_allstructs_45only, database37_allstructs_45only, database38_allstructs_45only,\n",
    "    database39_allstructs_45only, database40_allstructs_45only, database41_allstructs_45only,\n",
    "    database42_allstructs_45only, database43_allstructs_45only, database44_allstructs_45only,\n",
    "    database45_allstructs_45only, database46_allstructs_45only, database47_allstructs_45only,\n",
    "    database48_allstructs_45only\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seq, (temp, structs) in dataset.items():\n",
    "        # Only include if melting temperature is defined and above 20\n",
    "        if temp is not None:\n",
    "            combined_data[seq] = (temp, structs)\n",
    "\n",
    "combined_data2 = {}\n",
    "for i, j in combined_data.items():\n",
    "    if len(i) == len(j[1][-1]):\n",
    "        combined_data2[i] = j\n",
    "\n",
    "combined_data = combined_data2\n",
    "\n",
    "# Verify combined data size after filtering\n",
    "print(f\"Total size of combined dataset after filtering: {len(combined_data)}\")\n",
    "\n",
    "# Prepare data for train-test split\n",
    "sequences = list(combined_data.keys())\n",
    "labels = list(combined_data.values())\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% test)\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Create train and test sets as dictionaries\n",
    "train_set = {seq: label for seq, label in zip(train_sequences, train_labels)}\n",
    "test_set = {seq: label for seq, label in zip(test_sequences, test_labels)}\n",
    "\n",
    "# Display the split summary\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "994de9d2-83ea-4ca5-ba13-cdd20810a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, j in train_set.items():\n",
    "    if len(i) != len(j[1][-1]):\n",
    "        count +=1 \n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2423e75a-bf43-46fd-b32c-2d0084162646",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_sequences = [seq for seq in train_set.keys() if len(seq) == 45]\n",
    "top_structures = [train_set[seq][1] for seq in dna_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f745fd09-e181-473b-b036-bbd728f49f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777256\n"
     ]
    }
   ],
   "source": [
    "print(len(dna_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef21692e-1c22-4c59-9658-1ba8341a9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TATTTATCAGATCAATGGGGTATGAGCACAGTTAGTGGCCCGGCG',\n",
       " 'GAGCACGTCCGGTGTAAGTGTCTTGCCGGCCAGTACCGAACACGT',\n",
       " 'CTCAGAAATACACTCCTGCCGTGCGGATCAGAGGCACCATATATG',\n",
       " 'CGCTAATTCACAGGTCAACGATATTTAATCGTTTGCGCTGCATTT',\n",
       " 'GAGCCTGGGCGAGGGAGTGGCACATGATTTATCGACCTTAAATTC',\n",
       " 'CGATCGGTACAGCATGGTCGGAGGGCAAGGACAAACAAGTTTCGG',\n",
       " 'CGCTTAGGGCAGTTTAATCTCTGTTGTCCTTATATCGACCATACC',\n",
       " 'GCTACGATACACCCGGGACATGCTTAGATGTCCTATGTGCAACTT',\n",
       " 'TTATTATCCGGTGGGAGTTTATATCTCACTTAATGAGGGGCTCTC',\n",
       " 'TAAGATGCGTGAACAAGAGGGGAATTGGAGCGAAACGGAGCGCTA',\n",
       " 'TGTCCGTAATTCCGCTCCTCTACCTCCCGCCAGGACATACCGAAC',\n",
       " 'CTTTTGAGGGCCAGATGAAGACTCGAGTCCACGCATGTTGGGCCC',\n",
       " 'TGGAGCACGGTTTCTATAGGTTTGAGTCATTCTGCCTTTCAACCG',\n",
       " 'TTGTTATAACTGAAGTATCGTGCCATGTAAACACGCATTTCGGTA',\n",
       " 'TACACCCAAAGCCGCTAATGGATGCAGCTCGCATTGCCTGTCCGA',\n",
       " 'ATTAGGGGCAAGACTAACTGCGGGAGTTAGGGTGAGATCGCGAAA',\n",
       " 'AGGGGCGAATCGATTGGGTTGGGAATTTGTTTCGCGCCAGAGAAC',\n",
       " 'ATTGAACTTTTGTACGCGAATTATCAATGACGGTGGCAGATTTTT',\n",
       " 'CAAATTGGTGTTCCGCGTCAGTGACACGGCCTAATTGTTCAACCC',\n",
       " 'TAGTGCGAGGGTCCACAAAGTCTTACTGTCGTACTGTCCTAGTGG',\n",
       " 'AGTCGCCGCGAATTTAACGTTACCGCGCAGTCGCCAATTAGTAGA',\n",
       " 'CAAAATAACCAGGCCTCACGCTCAATGGGGGCCGCAGAGTGGGCC',\n",
       " 'TTACGACCCAAACCTAACCTGGCGCGGTTGCACCTTAGTGGAAGG',\n",
       " 'TTACAAAATTTACGCGCTGTTGCTTATGGATCGGATTCTGTGAGC',\n",
       " 'TTATAGTTTTGCTGACAGCGGACTGATATCAATACGAATGGTATT',\n",
       " 'AGCTAATACAAATGCGTCCTAATATTCGATTGATGGAAGGTTACC',\n",
       " 'ACCAGCCGCGCGGATCAGGGAACGCACTGTACCATGGGAAACTCT',\n",
       " 'TCTAACGACGAAAGAACCAACTCGTCTACCTCCGTTTATCCGGTG',\n",
       " 'GTAGTCACCACTAACTCTGAGAGGCAGCGTGCGTATGATGTCCGC',\n",
       " 'TGGTAGGGAGGTCCCGACGTTGCGTTTTACCAGGCGTCGGCGCTA']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_sequences[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17c93c17-7328-4b0f-909a-8eed3ffee0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.(((.....))).....((((.....(((.....)))))))....',\n",
       "  '...............((((.(...............).))))...',\n",
       "  '.(((.(.....).)))..........(((.....)))........',\n",
       "  '.(((.........)))..........(((.....)))........',\n",
       "  '.......((......)).........(((.....)))........',\n",
       "  '...........(((........))).(((.....)))........',\n",
       "  '......(((.(((.....))).))).(((.....)))........',\n",
       "  '.................((((.....(((.....)))))))....'],\n",
       " ['........(((((...........))))).....((.......))',\n",
       "  '...(((.....)))...((((.(...(((......))))))))..',\n",
       "  '......(.(((((...........))))))..((.....))....',\n",
       "  '...(((.....)))...((.....))(((......))).......',\n",
       "  '....((..(((((...........)))))...))..((....)).',\n",
       "  '.(((((...........))).))...(((......))).......',\n",
       "  '....((((.((((.....................))))...))))',\n",
       "  '....(((((((((...........)))))............))))',\n",
       "  '...(((.....)))..........(.(((......)))..)....',\n",
       "  '........(((((...........))))).......((....)).',\n",
       "  '...(((.....)))...((((.....(((......))).))))..',\n",
       "  '...(((...........)))......(((......))).......',\n",
       "  '........(((((...........)))))...((.....))....',\n",
       "  '...(((.....))).(((...)))..(((......))).......',\n",
       "  '...(((.....)))............(((......))).......'],\n",
       " ['...((.......))...(((.((.....))..)))..........',\n",
       "  '..........((....))..((((.........))))........',\n",
       "  '.................(((.((.....))..)))..((....))',\n",
       "  '.................(((.((.....))..)))..........',\n",
       "  '..(((..........)))..((((.........))))........'],\n",
       " ['....(((.((...((.((((((.....)))))).))..)).))).',\n",
       "  '(((.(............(((((.....)))))).)))........',\n",
       "  '........((......((((((.....))))))))..........',\n",
       "  '......(.....)...((((((.....)))))).((...))....',\n",
       "  '.((..........)).((((((.....)))))).((...))....',\n",
       "  '.((.............((((((.....))))))......))....',\n",
       "  '........(...)...((((((.....)))))).((...))....',\n",
       "  '..........((....((((((.....))))))))..........',\n",
       "  '....(((...(((...((((((.....))))))....))).))).',\n",
       "  '..((.......))...((((((.....)))))).((...))....',\n",
       "  '..........(((((.((((((.....)))))).)).))).....',\n",
       "  '..........(((..(((((((.....)))).)))..))).....',\n",
       "  '..........(((...((((((.....))))))....))).....',\n",
       "  '................((((((.....)))))).((...))....'],\n",
       " ['...(((.....)))....((...............))........',\n",
       "  '......(.((.........)).)...(((((.......)))))..',\n",
       "  '..((....))........(...)...(((((.......)))))..',\n",
       "  '......((.(((.(...............).))).))........',\n",
       "  '..(((.............)))....((....))............',\n",
       "  '......((.(((..(.(((...)))..)...))).))........',\n",
       "  '...(((.....)))....(...)...(((((.......)))))..',\n",
       "  '......((.(((.....((...)).......))).))........',\n",
       "  '..(((.............))).....(((((.......)))))..',\n",
       "  '......((.(((...................))).))........'],\n",
       " ['..............((.((...)).))...((......)).....',\n",
       "  '....((...((...))..))............(((....)))...',\n",
       "  '......(.....)...(((..........))).............',\n",
       "  '.........((...))(((..........))).............',\n",
       "  '...........((...........))......(((....)))...',\n",
       "  '(((((..........)))))............(((....)))...',\n",
       "  '................(((..........))).............',\n",
       "  '..............((.((...)).)).....(((....)))...'],\n",
       " ['.((.....))............((((.........))))......',\n",
       "  '.....((((((.............))))))...((.....))...',\n",
       "  '.........(((........)))..(((........)))......',\n",
       "  '.((.....))...............(((........)))......',\n",
       "  '.....((((((.............))))))...............'],\n",
       " ['.......((((...(((((((......))))))).))))......',\n",
       "  '((..((.......))((((((......)))))).....)).....',\n",
       "  '...(...).(((..(((((((......)))))))..)))......',\n",
       "  '....((.......))((((((......))))))...((...))..',\n",
       "  '...............((((((......))))))...((...))..',\n",
       "  '.........(((..(((((((......)))))))..)))......'],\n",
       " ['......(((....)))........((((.....)))).(.....)',\n",
       "  '..........((((((.......)))))).....(((.....)))',\n",
       "  '......(((....)))........((((.....))))........'],\n",
       " ['...........((...........))..((((........)))).',\n",
       "  '.....((......)).............((((........)))).',\n",
       "  '.............(((........))).((((........)))).']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_structures[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e9ad8-ec71-4953-a369-33e195ae0407",
   "metadata": {},
   "source": [
    "# Some Parameters Changed and Only Sequences 40-50 bp long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4370469-d5b7-4e42-9991-1450cb0b3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Sample weights stats:\n",
      "Min: 0.17018939554691315, Max: 0.3036707937717438, Mean: 0.2091214805841446\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "sequence_length = 45\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_shared_transformer_blocks = 12\n",
    "num_task_transformer_blocks = 6\n",
    "dropout_rate = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Pair-aware encoding functions\n",
    "def encode_dna_sequence(seq):\n",
    "    encoding = {'A': [1, 0], 'T': [-1, 0], 'C': [0, 1], 'G': [0, -1]}\n",
    "    return [encoding[base] for base in seq]\n",
    "\n",
    "def encode_structure(structure):\n",
    "    encoding = {'.': 0, '(': 1, ')': 2}\n",
    "    return [encoding[char] for char in structure]\n",
    "\n",
    "# Encode DNA sequences\n",
    "dna_sequences = [seq for seq in train_set.keys() if len(seq) == sequence_length]\n",
    "top_structures = [train_set[seq][1] for seq in dna_sequences]\n",
    "\n",
    "# Encode sequences and structures\n",
    "encoded_sequences = [encode_dna_sequence(seq) for seq in dna_sequences]\n",
    "encoded_structures = [encode_structure(structs[-1]) for structs in top_structures]\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(encoded_sequences, dtype=torch.float32)\n",
    "y_struct = torch.tensor(encoded_structures, dtype=torch.long)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_struct_train, y_struct_val = train_test_split(X, y_struct, test_size=0.2, random_state=42)\n",
    "\n",
    "\"\"\"\n",
    "class_counts = torch.tensor(\n",
    "    [sum(struct.count(char) for struct in [\"\".join(top_struct) for top_struct in top_structures]) for char in \".()\"], \n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Ensure no division by zero\n",
    "total_count = class_counts.sum()\n",
    "if total_count == 0 or any(class_counts == 0):\n",
    "    raise ValueError(\"Class counts are invalid, resulting in division by zero.\")\n",
    "\n",
    "# Compute class weights (Inverse frequency weighting)\n",
    "class_weights = total_count / (class_counts + 1e-5)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.clamp(class_weights, min=0)  # Ensure non-negative weights\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Create weighted sampler for the training set\n",
    "sample_weights = []\n",
    "for y in y_struct_train:\n",
    "    valid_indices = y  # Here we are using y directly as it represents the class labels for each base\n",
    "    weight = [class_weights[class_idx].item() for class_idx in valid_indices]\n",
    "    sample_weights.append(sum(weight) / len(weight))  # Average weight for the sequence\n",
    "\n",
    "# Convert to tensor and validate\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "sample_weights = torch.nan_to_num(sample_weights, nan=1.0, posinf=1.0, neginf=1.0)  # Replace NaN/infs with default value\n",
    "sample_weights = torch.clamp(sample_weights, min=0)  # Ensure weights are non-negative\"\"\"\n",
    "\n",
    "class_weights = torch.load(\"class_weights_only45s.pt\").to(device)\n",
    "sample_weights = torch.load(\"sample_weights_only45s.pt\").to(device)\n",
    "\n",
    "# Debugging: Print sample weights stats\n",
    "print(\"Sample weights stats:\")\n",
    "print(f\"Min: {sample_weights.min()}, Max: {sample_weights.max()}, Mean: {sample_weights.mean()}\")\n",
    "assert torch.all(sample_weights >= 0), \"Sample weights contain negative values!\"\n",
    "assert not torch.any(torch.isnan(sample_weights)), \"Sample weights contain NaN values!\"\n",
    "\n",
    "# Create sampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_struct_train)\n",
    "val_dataset = TensorDataset(X_val, y_struct_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f57ca6-0ce2-40d4-80ed-04fb55759afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "# Model Definition\n",
    "class TransformerEncoderBlockWithPairingAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.2, sequence_length=45):\n",
    "        super(TransformerEncoderBlockWithPairingAttention, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.pairing_bias = nn.Parameter(torch.randn(sequence_length, sequence_length))\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        q, k, v = x, x, x\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        seq_len = q.size(1)\n",
    "        pairing_bias_resized = self.pairing_bias[:seq_len, :seq_len]\n",
    "        pairing_bias_resized = pairing_bias_resized.unsqueeze(0).expand(x.size(0), -1, -1).to(x.device)\n",
    "        attn_scores = attn_scores + pairing_bias_resized\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        paired_attn_output = torch.matmul(attn_weights, v)\n",
    "        x = x + self.dropout1(attn_output + paired_attn_output)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.dropout2(self.ffn(x))\n",
    "        x = self.layernorm2(x)\n",
    "        return x\n",
    "\n",
    "class StructurePredictor(nn.Module):\n",
    "    def __init__(self, sequence_length=45, embedding_dim=64, num_heads=8, ff_dim=128, num_shared_blocks=2, num_task_blocks=2):\n",
    "        super(StructurePredictor, self).__init__()\n",
    "        self.dna_projection = nn.Linear(2, embedding_dim)  # Project [1, 0], [-1, 0], etc., to embedding space\n",
    "        self.positional_encoding = self.create_sinusoidal_positional_encoding(sequence_length, embedding_dim)\n",
    "        self.shared_transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlockWithPairingAttention(embedding_dim, num_heads, ff_dim, sequence_length=sequence_length)\n",
    "            for _ in range(num_shared_blocks)\n",
    "        ])\n",
    "        self.struct_transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlockWithPairingAttention(embedding_dim, num_heads, ff_dim, sequence_length=sequence_length)\n",
    "            for _ in range(num_task_blocks)\n",
    "        ])\n",
    "        self.structure_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 3)  # Predict 3 structure classes (., (, ))\n",
    "        )\n",
    "\n",
    "    def create_sinusoidal_positional_encoding(self, seq_len, d_model):\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pos_enc = torch.zeros(seq_len, d_model)\n",
    "        pos_enc[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(pos * div_term)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, encoded_structure=None):\n",
    "        device = x.device  # Get the device of input tensor\n",
    "        positional_encoding = self.positional_encoding.to(device)  # Move to same device\n",
    "    \n",
    "        x = self.dna_projection(x) + positional_encoding  # Project DNA encoding and add positional encoding\n",
    "        x = x.permute(1, 0, 2)  # Permute for transformer compatibility\n",
    "    \n",
    "        # Shared Transformer blocks\n",
    "        for block in self.shared_transformer_blocks:\n",
    "            x = block(x)\n",
    "    \n",
    "        # Output layers\n",
    "        output = self.structure_head(x.permute(1, 0, 2))  # Permute back to original shape\n",
    "        return output\n",
    "\n",
    "class RewardedThermodynamicallyBalancedCategoricalCrossEntropy(nn.Module):\n",
    "    def __init__(self, weights, ignore_index=-1, pairing_penalty=0.2, thermo_penalty=0.3, specificity_reward=0.05):\n",
    "        super(RewardedThermodynamicallyBalancedCategoricalCrossEntropy, self).__init__()\n",
    "        self.weights = weights  # Use the full weights tensor without excluding padding weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.pairing_penalty = pairing_penalty\n",
    "        self.thermo_penalty = thermo_penalty\n",
    "        self.specificity_reward = specificity_reward\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Create a mask to exclude ignored indices\n",
    "        mask = (y_true != self.ignore_index)\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "\n",
    "        if y_true.numel() == 0:  # Handle empty tensors gracefully\n",
    "            return torch.tensor(0.0, requires_grad=True, device=y_pred.device)\n",
    "\n",
    "        # Weighted categorical cross-entropy\n",
    "        y_true_one_hot = F.one_hot(y_true, num_classes=3).float()\n",
    "        log_probs = F.log_softmax(y_pred, dim=-1)\n",
    "        loss = -torch.sum(self.weights * y_true_one_hot * log_probs, dim=-1).mean()\n",
    "\n",
    "        # Pairing imbalance penalty\n",
    "        pred_labels = torch.argmax(y_pred, dim=-1)\n",
    "        open_count = (pred_labels == 1).sum()\n",
    "        close_count = (pred_labels == 2).sum()\n",
    "        imbalance_penalty = self.pairing_penalty * torch.abs(open_count - close_count).float()\n",
    "\n",
    "        # Thermodynamic penalty for mismatches\n",
    "        mismatch_penalty = torch.sum((pred_labels == 1) & (y_true == 2)) * self.thermo_penalty\n",
    "\n",
    "        # Specificity reward for correct pair predictions\n",
    "        correct_pairings = ((pred_labels == y_true) & ((y_true == 1) | (y_true == 2))).sum()\n",
    "        specificity_reward = self.specificity_reward * correct_pairings.float()\n",
    "\n",
    "        return loss + imbalance_penalty + mismatch_penalty - specificity_reward\n",
    "\n",
    "\n",
    "# Instantiate model and loss function\n",
    "model = StructurePredictor(sequence_length, embedding_dim, num_heads, ff_dim, num_shared_transformer_blocks, num_task_transformer_blocks).to(device)\n",
    "loss_fn_struct = RewardedThermodynamicallyBalancedCategoricalCrossEntropy(class_weights)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\n",
    "\n",
    "# Training loop with early stopping and metrics\n",
    "best_val_loss = float('inf')\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "def calculate_class_metrics(y_true, y_pred, num_classes=3):\n",
    "    metrics = {}\n",
    "    for class_idx in range(num_classes):\n",
    "        tp = ((y_pred == class_idx) & (y_true == class_idx)).sum().item()\n",
    "        fp = ((y_pred == class_idx) & (y_true != class_idx)).sum().item()\n",
    "        fn = ((y_pred != class_idx) & (y_true == class_idx)).sum().item()\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        metrics[class_idx] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    return metrics\n",
    "\n",
    "def print_validation_metrics(val_metrics, printer=True):\n",
    "    # Initialize a variable to store the sum of F1 scores\n",
    "    total_f1 = 0\n",
    "    num_classes = len(val_metrics)\n",
    "    \n",
    "    # Print metrics for each class\n",
    "    for cls, metrics in val_metrics.items():\n",
    "        if printer:\n",
    "            print(f\"Class {cls} - Precision: {metrics['precision']:.4f}, \"\n",
    "                  f\"Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
    "        total_f1 += metrics['f1']\n",
    "    \n",
    "    # Calculate the average F1 score\n",
    "    avg_f1 = total_f1 / num_classes if num_classes > 0 else 0.0\n",
    "    \n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2cad421-5c8e-4439-b0c1-a8ac50b5800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Save sample weights\\ntorch.save(sample_weights, \"sample_weights_evenevenmoredata.pt\")\\n\\n# Load sample weights\\ntorch.save(class_weights, \"class_weights_evenevenmoredata.pt\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Save sample weights\n",
    "torch.save(sample_weights, \"sample_weights_evenevenmoredata.pt\")\n",
    "\n",
    "# Load sample weights\n",
    "torch.save(class_weights, \"class_weights_evenevenmoredata.pt\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b1c9865-6de3-4c08-be14-0b42aa449558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\\nmodel_path = \"12-2_integerenc.pth\"\\nmodel.load_state_dict(torch.load(model_path, map_location=device))'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run this cell if the gradients explode!\n",
    "\"\"\"\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\n",
    "model_path = \"12-2_integerenc.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb2ca60-346a-4489-92cf-57934063aa03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 9.1072\n",
      "Learning Rate: 8.136061215030879e-06\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0134\n",
      "shared_transformer_blocks: 0.0323\n",
      "structure_head: 0.1190\n",
      "Class 0 - Precision: 0.6170, Recall: 0.5577, F1-Score: 0.5859\n",
      "Class 1 - Precision: 0.3333, Recall: 0.2105, F1-Score: 0.2581\n",
      "Class 2 - Precision: 0.3871, Recall: 0.6316, F1-Score: 0.4800\n",
      "Validation Loss: 3.5748, Accuracy: 0.4644, F1: 0.4413\n",
      "Best model saved with validation loss: 3.5748\n",
      "\n",
      "\n",
      "Epoch 2, Training Loss: 8.6278\n",
      "Learning Rate: 8.543859179826856e-06\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0092\n",
      "shared_transformer_blocks: 0.0168\n",
      "structure_head: 0.0618\n",
      "Class 0 - Precision: 0.6047, Recall: 0.5000, F1-Score: 0.5474\n",
      "Class 1 - Precision: 0.2941, Recall: 0.2632, F1-Score: 0.2778\n",
      "Class 2 - Precision: 0.4000, Recall: 0.6316, F1-Score: 0.4898\n",
      "Validation Loss: 2.6202, Accuracy: 0.4089, F1: 0.4383\n",
      "Best model saved with validation loss: 2.6202\n",
      "\n",
      "\n",
      "Epoch 3, Training Loss: 2.7211\n",
      "Learning Rate: 9.222237946750927e-06\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0081\n",
      "shared_transformer_blocks: 0.0154\n",
      "structure_head: 0.0583\n",
      "Class 0 - Precision: 0.6047, Recall: 0.5000, F1-Score: 0.5474\n",
      "Class 1 - Precision: 0.2941, Recall: 0.2632, F1-Score: 0.2778\n",
      "Class 2 - Precision: 0.4000, Recall: 0.6316, F1-Score: 0.4898\n",
      "Validation Loss: 2.6171, Accuracy: 0.4311, F1: 0.4383\n",
      "Best model saved with validation loss: 2.6171\n",
      "\n",
      "\n",
      "Epoch 4, Training Loss: 3.5049\n",
      "Learning Rate: 1.0169274577484803e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0115\n",
      "shared_transformer_blocks: 0.0245\n",
      "structure_head: 0.0952\n",
      "Class 0 - Precision: 0.5641, Recall: 0.4231, F1-Score: 0.4835\n",
      "Class 1 - Precision: 0.2400, Recall: 0.3158, F1-Score: 0.2727\n",
      "Class 2 - Precision: 0.4231, Recall: 0.5789, F1-Score: 0.4889\n",
      "Validation Loss: 0.5133, Accuracy: 0.4333, F1: 0.4150\n",
      "Best model saved with validation loss: 0.5133\n",
      "\n",
      "\n",
      "Epoch 5, Training Loss: 2.5389\n",
      "Learning Rate: 1.1382284593806468e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0080\n",
      "shared_transformer_blocks: 0.0141\n",
      "structure_head: 0.0493\n",
      "Class 0 - Precision: 0.5676, Recall: 0.4038, F1-Score: 0.4719\n",
      "Class 1 - Precision: 0.2593, Recall: 0.3684, F1-Score: 0.3043\n",
      "Class 2 - Precision: 0.4231, Recall: 0.5789, F1-Score: 0.4889\n",
      "Validation Loss: 0.4605, Accuracy: 0.3733, F1: 0.4217\n",
      "Best model saved with validation loss: 0.4605\n",
      "\n",
      "\n",
      "Epoch 6, Training Loss: 2.6579\n",
      "Learning Rate: 1.2857829587035395e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0090\n",
      "shared_transformer_blocks: 0.0213\n",
      "structure_head: 0.0821\n",
      "Class 0 - Precision: 0.6296, Recall: 0.3269, F1-Score: 0.4304\n",
      "Class 1 - Precision: 0.2703, Recall: 0.5263, F1-Score: 0.3571\n",
      "Class 2 - Precision: 0.4231, Recall: 0.5789, F1-Score: 0.4889\n",
      "Validation Loss: 3.2074, Accuracy: 0.3933, F1: 0.4255\n",
      "\n",
      "\n",
      "Epoch 7, Training Loss: 13.1204\n",
      "Learning Rate: 1.4591726964575502e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0120\n",
      "shared_transformer_blocks: 0.0265\n",
      "structure_head: 0.0969\n",
      "Class 0 - Precision: 0.6000, Recall: 0.2885, F1-Score: 0.3896\n",
      "Class 1 - Precision: 0.2564, Recall: 0.5263, F1-Score: 0.3448\n",
      "Class 2 - Precision: 0.4231, Recall: 0.5789, F1-Score: 0.4889\n",
      "Validation Loss: 3.6057, Accuracy: 0.3200, F1: 0.4078\n",
      "\n",
      "\n",
      "Epoch 8, Training Loss: 12.1664\n",
      "Learning Rate: 1.6579061805928314e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0114\n",
      "shared_transformer_blocks: 0.0292\n",
      "structure_head: 0.1054\n",
      "Class 0 - Precision: 0.6154, Recall: 0.3077, F1-Score: 0.4103\n",
      "Class 1 - Precision: 0.2632, Recall: 0.5263, F1-Score: 0.3509\n",
      "Class 2 - Precision: 0.4231, Recall: 0.5789, F1-Score: 0.4889\n",
      "Validation Loss: 3.4051, Accuracy: 0.3200, F1: 0.4167\n",
      "\n",
      "\n",
      "Epoch 9, Training Loss: 9.7294\n",
      "Learning Rate: 1.881420079456915e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0091\n",
      "shared_transformer_blocks: 0.0193\n",
      "structure_head: 0.0784\n",
      "Class 0 - Precision: 0.5938, Recall: 0.3654, F1-Score: 0.4524\n",
      "Class 1 - Precision: 0.2667, Recall: 0.4211, F1-Score: 0.3265\n",
      "Class 2 - Precision: 0.4286, Recall: 0.6316, F1-Score: 0.5106\n",
      "Validation Loss: 0.5546, Accuracy: 0.4111, F1: 0.4298\n",
      "\n",
      "\n",
      "Epoch 10, Training Loss: 4.0546\n",
      "Learning Rate: 2.1290808186194625e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0123\n",
      "shared_transformer_blocks: 0.0274\n",
      "structure_head: 0.1026\n",
      "Class 0 - Precision: 0.5676, Recall: 0.4038, F1-Score: 0.4719\n",
      "Class 1 - Precision: 0.2400, Recall: 0.3158, F1-Score: 0.2727\n",
      "Class 2 - Precision: 0.4286, Recall: 0.6316, F1-Score: 0.5106\n",
      "Validation Loss: 0.5565, Accuracy: 0.3933, F1: 0.4184\n",
      "\n",
      "\n",
      "Epoch 11, Training Loss: 5.9583\n",
      "Learning Rate: 2.4001863768078577e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0111\n",
      "shared_transformer_blocks: 0.0272\n",
      "structure_head: 0.1055\n",
      "Class 0 - Precision: 0.5789, Recall: 0.4231, F1-Score: 0.4889\n",
      "Class 1 - Precision: 0.2174, Recall: 0.2632, F1-Score: 0.2381\n",
      "Class 2 - Precision: 0.4483, Recall: 0.6842, F1-Score: 0.5417\n",
      "Validation Loss: 1.1567, Accuracy: 0.4111, F1: 0.4229\n",
      "\n",
      "\n",
      "Epoch 12, Training Loss: 0.4414\n",
      "Learning Rate: 2.6939682758627515e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0079\n",
      "shared_transformer_blocks: 0.0194\n",
      "structure_head: 0.0726\n",
      "Class 0 - Precision: 0.6190, Recall: 0.5000, F1-Score: 0.5532\n",
      "Class 1 - Precision: 0.2353, Recall: 0.2105, F1-Score: 0.2222\n",
      "Class 2 - Precision: 0.4516, Recall: 0.7368, F1-Score: 0.5600\n",
      "Validation Loss: 2.4556, Accuracy: 0.4822, F1: 0.4451\n",
      "\n",
      "\n",
      "Epoch 13, Training Loss: 3.0380\n",
      "Learning Rate: 3.0095937590729016e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0118\n",
      "shared_transformer_blocks: 0.0298\n",
      "structure_head: 0.1113\n",
      "Class 0 - Precision: 0.5652, Recall: 0.2500, F1-Score: 0.3467\n",
      "Class 1 - Precision: 0.2414, Recall: 0.3684, F1-Score: 0.2917\n",
      "Class 2 - Precision: 0.3947, Recall: 0.7895, F1-Score: 0.5263\n",
      "Validation Loss: 1.8519, Accuracy: 0.4644, F1: 0.3882\n",
      "\n",
      "\n",
      "Epoch 14, Training Loss: 5.1393\n",
      "Learning Rate: 3.3461681517145544e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0104\n",
      "shared_transformer_blocks: 0.0295\n",
      "structure_head: 0.1158\n",
      "Class 0 - Precision: 0.6154, Recall: 0.1538, F1-Score: 0.2462\n",
      "Class 1 - Precision: 0.2727, Recall: 0.4737, F1-Score: 0.3462\n",
      "Class 2 - Precision: 0.3636, Recall: 0.8421, F1-Score: 0.5079\n",
      "Validation Loss: 2.0969, Accuracy: 0.4244, F1: 0.3667\n",
      "\n",
      "\n",
      "Epoch 15, Training Loss: -0.9709\n",
      "Learning Rate: 3.702737397104147e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0056\n",
      "shared_transformer_blocks: 0.0133\n",
      "structure_head: 0.0494\n",
      "Class 0 - Precision: 0.6667, Recall: 0.0385, F1-Score: 0.0727\n",
      "Class 1 - Precision: 0.2973, Recall: 0.5789, F1-Score: 0.3929\n",
      "Class 2 - Precision: 0.3200, Recall: 0.8421, F1-Score: 0.4638\n",
      "Validation Loss: 2.3937, Accuracy: 0.3978, F1: 0.3098\n",
      "\n",
      "\n",
      "Epoch 16, Training Loss: -0.0874\n",
      "Learning Rate: 4.078290760975629e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0079\n",
      "shared_transformer_blocks: 0.0194\n",
      "structure_head: 0.0732\n",
      "Class 0 - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Class 1 - Precision: 0.2927, Recall: 0.6316, F1-Score: 0.4000\n",
      "Class 2 - Precision: 0.3265, Recall: 0.8421, F1-Score: 0.4706\n",
      "Validation Loss: 1.3422, Accuracy: 0.3311, F1: 0.2902\n",
      "\n",
      "\n",
      "Epoch 17, Training Loss: 9.4260\n",
      "Learning Rate: 4.471763696516559e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0081\n",
      "shared_transformer_blocks: 0.0220\n",
      "structure_head: 0.0810\n",
      "Class 0 - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Class 1 - Precision: 0.2927, Recall: 0.6316, F1-Score: 0.4000\n",
      "Class 2 - Precision: 0.3265, Recall: 0.8421, F1-Score: 0.4706\n",
      "Validation Loss: 1.3414, Accuracy: 0.2867, F1: 0.2902\n",
      "\n",
      "\n",
      "Epoch 18, Training Loss: 0.5245\n",
      "Learning Rate: 4.882040861941696e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0062\n",
      "shared_transformer_blocks: 0.0144\n",
      "structure_head: 0.0554\n",
      "Class 0 - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Class 1 - Precision: 0.3182, Recall: 0.7368, F1-Score: 0.4444\n",
      "Class 2 - Precision: 0.3261, Recall: 0.7895, F1-Score: 0.4615\n",
      "Validation Loss: 0.3912, Accuracy: 0.3733, F1: 0.3020\n",
      "Best model saved with validation loss: 0.3912\n",
      "\n",
      "\n",
      "Epoch 19, Training Loss: 2.5628\n",
      "Learning Rate: 5.307959282050475e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0083\n",
      "shared_transformer_blocks: 0.0216\n",
      "structure_head: 0.0814\n",
      "Class 0 - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Class 1 - Precision: 0.3043, Recall: 0.7368, F1-Score: 0.4308\n",
      "Class 2 - Precision: 0.3182, Recall: 0.7368, F1-Score: 0.4444\n",
      "Validation Loss: 0.7417, Accuracy: 0.3289, F1: 0.2917\n",
      "\n",
      "\n",
      "Epoch 20, Training Loss: -0.0388\n",
      "Learning Rate: 5.748311644806559e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0060\n",
      "shared_transformer_blocks: 0.0150\n",
      "structure_head: 0.0531\n",
      "Class 0 - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Class 1 - Precision: 0.3043, Recall: 0.7368, F1-Score: 0.4308\n",
      "Class 2 - Precision: 0.3182, Recall: 0.7368, F1-Score: 0.4444\n",
      "Validation Loss: 0.7433, Accuracy: 0.3333, F1: 0.2917\n",
      "\n",
      "\n",
      "Epoch 21, Training Loss: -1.2359\n",
      "Learning Rate: 6.201849723594987e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0047\n",
      "shared_transformer_blocks: 0.0110\n",
      "structure_head: 0.0378\n",
      "Class 0 - Precision: 0.8571, Recall: 0.1154, F1-Score: 0.2034\n",
      "Class 1 - Precision: 0.3333, Recall: 0.7368, F1-Score: 0.4590\n",
      "Class 2 - Precision: 0.3171, Recall: 0.6842, F1-Score: 0.4333\n",
      "Validation Loss: 0.5963, Accuracy: 0.3911, F1: 0.3652\n",
      "\n",
      "\n",
      "Epoch 22, Training Loss: 7.3980\n",
      "Learning Rate: 6.66728791545612e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0072\n",
      "shared_transformer_blocks: 0.0191\n",
      "structure_head: 0.0699\n",
      "Class 0 - Precision: 0.6667, Recall: 0.3462, F1-Score: 0.4557\n",
      "Class 1 - Precision: 0.3103, Recall: 0.4737, F1-Score: 0.3750\n",
      "Class 2 - Precision: 0.3824, Recall: 0.6842, F1-Score: 0.4906\n",
      "Validation Loss: 1.0532, Accuracy: 0.4200, F1: 0.4404\n",
      "\n",
      "\n",
      "Epoch 23, Training Loss: 1.4803\n",
      "Learning Rate: 7.143306885266912e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0088\n",
      "shared_transformer_blocks: 0.0199\n",
      "structure_head: 0.0738\n",
      "Class 0 - Precision: 0.6471, Recall: 0.4231, F1-Score: 0.5116\n",
      "Class 1 - Precision: 0.2917, Recall: 0.3684, F1-Score: 0.3256\n",
      "Class 2 - Precision: 0.3750, Recall: 0.6316, F1-Score: 0.4706\n",
      "Validation Loss: 1.8078, Accuracy: 0.4978, F1: 0.4359\n",
      "\n",
      "\n",
      "Epoch 24, Training Loss: 4.9960\n",
      "Learning Rate: 7.62855730553964e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0052\n",
      "shared_transformer_blocks: 0.0113\n",
      "structure_head: 0.0405\n",
      "Class 0 - Precision: 0.6222, Recall: 0.5385, F1-Score: 0.5773\n",
      "Class 1 - Precision: 0.3684, Recall: 0.3684, F1-Score: 0.3684\n",
      "Class 2 - Precision: 0.3077, Recall: 0.4211, F1-Score: 0.3556\n",
      "Validation Loss: 1.5145, Accuracy: 0.4756, F1: 0.4338\n",
      "\n",
      "\n",
      "Epoch 25, Training Loss: 4.9481\n",
      "Learning Rate: 8.121663681237296e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0066\n",
      "shared_transformer_blocks: 0.0154\n",
      "structure_head: 0.0555\n",
      "Class 0 - Precision: 0.6087, Recall: 0.5385, F1-Score: 0.5714\n",
      "Class 1 - Precision: 0.3684, Recall: 0.3684, F1-Score: 0.3684\n",
      "Class 2 - Precision: 0.2800, Recall: 0.3684, F1-Score: 0.3182\n",
      "Validation Loss: 1.3677, Accuracy: 0.5444, F1: 0.4193\n",
      "\n",
      "\n",
      "Epoch 26, Training Loss: 2.0511\n",
      "Learning Rate: 8.621228248763702e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0076\n",
      "shared_transformer_blocks: 0.0215\n",
      "structure_head: 0.0836\n",
      "Class 0 - Precision: 0.6000, Recall: 0.3462, F1-Score: 0.4390\n",
      "Class 1 - Precision: 0.3429, Recall: 0.6316, F1-Score: 0.4444\n",
      "Class 2 - Precision: 0.2800, Recall: 0.3684, F1-Score: 0.3182\n",
      "Validation Loss: 2.2140, Accuracy: 0.4822, F1: 0.4006\n",
      "\n",
      "\n",
      "Epoch 27, Training Loss: 4.1631\n",
      "Learning Rate: 9.12583493807627e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0086\n",
      "shared_transformer_blocks: 0.0173\n",
      "structure_head: 0.0658\n",
      "Class 0 - Precision: 0.7222, Recall: 0.2500, F1-Score: 0.3714\n",
      "Class 1 - Precision: 0.3415, Recall: 0.7368, F1-Score: 0.4667\n",
      "Class 2 - Precision: 0.3548, Recall: 0.5789, F1-Score: 0.4400\n",
      "Validation Loss: 2.2081, Accuracy: 0.4622, F1: 0.4260\n",
      "\n",
      "\n",
      "Epoch 28, Training Loss: 9.1192\n",
      "Learning Rate: 9.634053386690403e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0099\n",
      "shared_transformer_blocks: 0.0220\n",
      "structure_head: 0.0785\n",
      "Class 0 - Precision: 0.8333, Recall: 0.0962, F1-Score: 0.1724\n",
      "Class 1 - Precision: 0.3409, Recall: 0.7895, F1-Score: 0.4762\n",
      "Class 2 - Precision: 0.3250, Recall: 0.6842, F1-Score: 0.4407\n",
      "Validation Loss: 1.1550, Accuracy: 0.3933, F1: 0.3631\n",
      "\n",
      "\n",
      "Epoch 29, Training Loss: 2.8532\n",
      "Learning Rate: 0.00010144442994197362\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0075\n",
      "shared_transformer_blocks: 0.0206\n",
      "structure_head: 0.0816\n",
      "Class 0 - Precision: 1.0000, Recall: 0.0769, F1-Score: 0.1429\n",
      "Class 1 - Precision: 0.3409, Recall: 0.7895, F1-Score: 0.4762\n",
      "Class 2 - Precision: 0.3333, Recall: 0.7368, F1-Score: 0.4590\n",
      "Validation Loss: 0.7068, Accuracy: 0.3511, F1: 0.3594\n",
      "\n",
      "\n",
      "Epoch 30, Training Loss: 1.0859\n",
      "Learning Rate: 0.00010655557005802638\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0124\n",
      "shared_transformer_blocks: 0.0385\n",
      "structure_head: 0.1577\n",
      "Class 0 - Precision: 0.6667, Recall: 0.1154, F1-Score: 0.1967\n",
      "Class 1 - Precision: 0.3333, Recall: 0.6842, F1-Score: 0.4483\n",
      "Class 2 - Precision: 0.3095, Recall: 0.6842, F1-Score: 0.4262\n",
      "Validation Loss: 0.7581, Accuracy: 0.2911, F1: 0.3571\n",
      "\n",
      "\n",
      "Epoch 31, Training Loss: -0.7475\n",
      "Learning Rate: 0.000111659466133096\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0066\n",
      "shared_transformer_blocks: 0.0120\n",
      "structure_head: 0.0417\n",
      "Class 0 - Precision: 0.6190, Recall: 0.2500, F1-Score: 0.3562\n",
      "Class 1 - Precision: 0.3103, Recall: 0.4737, F1-Score: 0.3750\n",
      "Class 2 - Precision: 0.3250, Recall: 0.6842, F1-Score: 0.4407\n",
      "Validation Loss: 2.2645, Accuracy: 0.4156, F1: 0.3906\n",
      "\n",
      "\n",
      "Epoch 32, Training Loss: 4.9468\n",
      "Learning Rate: 0.0001167416506192373\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0061\n",
      "shared_transformer_blocks: 0.0143\n",
      "structure_head: 0.0470\n",
      "Class 0 - Precision: 0.6341, Recall: 0.5000, F1-Score: 0.5591\n",
      "Class 1 - Precision: 0.3571, Recall: 0.5263, F1-Score: 0.4255\n",
      "Class 2 - Precision: 0.3810, Recall: 0.4211, F1-Score: 0.4000\n",
      "Validation Loss: 1.3733, Accuracy: 0.4689, F1: 0.4616\n",
      "\n",
      "\n",
      "Epoch 33, Training Loss: 5.9014\n",
      "Learning Rate: 0.00012178771751236296\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0070\n",
      "shared_transformer_blocks: 0.0167\n",
      "structure_head: 0.0656\n",
      "Class 0 - Precision: 0.5778, Recall: 0.5000, F1-Score: 0.5361\n",
      "Class 1 - Precision: 0.3571, Recall: 0.5263, F1-Score: 0.4255\n",
      "Class 2 - Precision: 0.2941, Recall: 0.2632, F1-Score: 0.2778\n",
      "Validation Loss: 2.3246, Accuracy: 0.5200, F1: 0.4131\n",
      "\n",
      "\n",
      "Epoch 34, Training Loss: 3.2894\n",
      "Learning Rate: 0.00012678336318762705\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0048\n",
      "shared_transformer_blocks: 0.0102\n",
      "structure_head: 0.0335\n",
      "Class 0 - Precision: 0.5682, Recall: 0.4808, F1-Score: 0.5208\n",
      "Class 1 - Precision: 0.3226, Recall: 0.5263, F1-Score: 0.4000\n",
      "Class 2 - Precision: 0.2667, Recall: 0.2105, F1-Score: 0.2353\n",
      "Validation Loss: 3.9729, Accuracy: 0.5244, F1: 0.3854\n",
      "\n",
      "\n",
      "Epoch 35, Training Loss: 5.1848\n",
      "Learning Rate: 0.0001317144269446036\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0092\n",
      "shared_transformer_blocks: 0.0229\n",
      "structure_head: 0.0987\n",
      "Class 0 - Precision: 0.6667, Recall: 0.3462, F1-Score: 0.4557\n",
      "Class 1 - Precision: 0.3243, Recall: 0.6316, F1-Score: 0.4286\n",
      "Class 2 - Precision: 0.3846, Recall: 0.5263, F1-Score: 0.4444\n",
      "Validation Loss: 2.8562, Accuracy: 0.5311, F1: 0.4429\n",
      "\n",
      "\n",
      "Epoch 36, Training Loss: 5.7879\n",
      "Learning Rate: 0.00013656693114733092\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0052\n",
      "shared_transformer_blocks: 0.0126\n",
      "structure_head: 0.0451\n",
      "Class 0 - Precision: 0.8571, Recall: 0.1154, F1-Score: 0.2034\n",
      "Class 1 - Precision: 0.3415, Recall: 0.7368, F1-Score: 0.4667\n",
      "Class 2 - Precision: 0.3333, Recall: 0.7368, F1-Score: 0.4590\n",
      "Validation Loss: 0.5497, Accuracy: 0.4489, F1: 0.3764\n",
      "\n",
      "\n",
      "Epoch 37, Training Loss: -0.5085\n",
      "Learning Rate: 0.00014132712084543878\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0064\n",
      "shared_transformer_blocks: 0.0133\n",
      "structure_head: 0.0524\n",
      "Class 0 - Precision: 0.7273, Recall: 0.1538, F1-Score: 0.2540\n",
      "Class 1 - Precision: 0.3243, Recall: 0.6316, F1-Score: 0.4286\n",
      "Class 2 - Precision: 0.3333, Recall: 0.7368, F1-Score: 0.4590\n",
      "Validation Loss: 1.4499, Accuracy: 0.3867, F1: 0.3805\n",
      "\n",
      "\n",
      "Epoch 38, Training Loss: 4.4128\n",
      "Learning Rate: 0.00014598150276405012\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0105\n",
      "shared_transformer_blocks: 0.0207\n",
      "structure_head: 0.0851\n",
      "Class 0 - Precision: 1.0000, Recall: 0.0962, F1-Score: 0.1754\n",
      "Class 1 - Precision: 0.3488, Recall: 0.7895, F1-Score: 0.4839\n",
      "Class 2 - Precision: 0.3333, Recall: 0.7368, F1-Score: 0.4590\n",
      "Validation Loss: 0.5002, Accuracy: 0.4311, F1: 0.3728\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, train_accuracies = [], [], []\n",
    "val_losses, val_f1s, val_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    layer_grad_norms = {}  # Dictionary to accumulate grad norms by layer\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_X, batch_y_struct in train_dataloader:\n",
    "        batch_X, batch_y_struct = batch_X.to(device), batch_y_struct.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred_struct = model(batch_X, batch_y_struct)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = loss_fn_struct(pred_struct.view(-1, 3), batch_y_struct.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradient norms by layer name\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                layer_name = name.split('.')[0]  # Use only the first part as layer name\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                if layer_name not in layer_grad_norms:\n",
    "                    layer_grad_norms[layer_name] = []\n",
    "                layer_grad_norms[layer_name].append(grad_norm)\n",
    "\n",
    "        # Gradient clipping\n",
    "        nn_utils.clip_grad_value_(model.parameters(), clip_value=0.1)\n",
    "\n",
    "        # Optimizer and scheduler step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Collect true and predicted labels\n",
    "        y_true = batch_y_struct.view(-1)\n",
    "        y_pred = torch.argmax(pred_struct.view(-1, 3), dim=-1)\n",
    "        all_true_labels.append(y_true.cpu())\n",
    "        all_predicted_labels.append(y_pred.cpu())\n",
    "\n",
    "        # Calculate correct predictions\n",
    "        correct_preds += (y_pred == y_true).sum().item()\n",
    "        total_preds += y_true.size(0)\n",
    "\n",
    "    # Calculate training metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    all_true_labels = torch.cat(all_true_labels)\n",
    "    all_predicted_labels = torch.cat(all_predicted_labels)\n",
    "    train_metrics = calculate_class_metrics(all_true_labels, all_predicted_labels)\n",
    "    avg_train_f1 = print_validation_metrics(train_metrics, printer=False)\n",
    "    accuracy = correct_preds / total_preds\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    train_f1s.append(avg_train_f1)\n",
    "\n",
    "    # Calculate average gradient norms per layer\n",
    "    avg_layer_grad_norms = {layer: sum(norms) / len(norms) for layer, norms in layer_grad_norms.items()}\n",
    "\n",
    "    # Print epoch summary with average layer gradient norms\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    print(\"Average Gradient Norms per Layer:\")\n",
    "    for layer, avg_norm in avg_layer_grad_norms.items():\n",
    "        print(f\"{layer}: {avg_norm:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y_struct in val_dataloader:\n",
    "            val_X, val_y_struct = val_X.to(device), val_y_struct.to(device)\n",
    "            val_pred_struct = model(val_X, val_y_struct)\n",
    "            val_loss += loss_fn_struct(val_pred_struct.view(-1, 3), val_y_struct.view(-1)).item()\n",
    "\n",
    "            # Collect true and predicted labels\n",
    "            y_true = val_y_struct.view(-1)\n",
    "            y_pred = torch.argmax(val_pred_struct.view(-1, 3), dim=-1)\n",
    "            all_y_true.append(y_true.cpu())\n",
    "            all_y_pred.append(y_pred.cpu())\n",
    "\n",
    "            # Calculate correct predictions\n",
    "            correct_preds += (y_pred == y_true).sum().item()\n",
    "            total_preds += y_true.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    all_y_true = torch.cat(all_y_true)\n",
    "    all_y_pred = torch.cat(all_y_pred)\n",
    "    val_metrics = calculate_class_metrics(all_y_true, all_y_pred)\n",
    "    avg_val_f1 = print_validation_metrics(val_metrics)\n",
    "    val_accuracy = correct_preds / total_preds\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1s.append(avg_val_f1)\n",
    "\n",
    "    # Print validation summary\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {avg_val_f1:.4f}\")\n",
    "\n",
    "    # Check for early stopping and save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"12-2_2D_vecenc.pth\")\n",
    "        print(f\"Best model saved with validation loss: {avg_val_loss:.4f}\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a5367-3079-48d4-96d7-5b3d36f853c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to save each of the lists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Save lists to CSV files with filenames related to the model name\n",
    "model_name = \"12-2_2D_vecenc\"\n",
    "\n",
    "# Create a dictionary for each list to be saved\n",
    "data_to_save = {\n",
    "    f\"{model_name}_train_losses.csv\": train_losses,\n",
    "    f\"{model_name}_train_f1s.csv\": train_f1s,\n",
    "    f\"{model_name}_train_accuracies.csv\": train_accuracies,\n",
    "    f\"{model_name}_val_losses.csv\": val_losses,\n",
    "    f\"{model_name}_val_f1s.csv\": val_f1s,\n",
    "    f\"{model_name}_val_accuracies.csv\": val_accuracies,\n",
    "}\n",
    "\n",
    "# Loop through each list and save it as a CSV\n",
    "for filename, data_list in data_to_save.items():\n",
    "    # Convert list to DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=[filename.split('_')[-1].split('.')[0]])  # Use the metric name as column\n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "print(\"Metrics saved to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de409858-6d6e-4850-8c20-1286f4ad3942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0381d9-c6cb-4cdf-996e-722e4bb74059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2541a-9f9f-48aa-b279-eb167969d587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecba93-c411-4a27-97fc-f7aa89d84190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bc70f-6404-4432-8519-f5b8ae844ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_path = \"12-2_2D_vecenc.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Function to decode structure\n",
    "def decode_structure(encoded):\n",
    "    structure_mapping = {0: '.', 1: '(', 2: ')', 3: '-'}  # '-' represents padding\n",
    "    return ''.join([structure_mapping[code.item()] for code in encoded])\n",
    "\n",
    "def enforce_symmetry_and_minimum_distance(pred_structure, min_distance=3):\n",
    "    stack = []\n",
    "    for i, char in enumerate(pred_structure):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                opening_index = stack.pop()\n",
    "                # Enforce minimum distance rule\n",
    "                if i - opening_index - 1 < min_distance or ''.join(pred_structure[opening_index + 1:i]).count('.') < min_distance:\n",
    "                    # Replace invalid pair with dots\n",
    "                    pred_structure[opening_index] = '.'\n",
    "                    pred_structure[i] = '.'\n",
    "    # Replace unmatched '(' with '.'\n",
    "    for i in stack:\n",
    "        pred_structure[i] = '.'\n",
    "    return pred_structure\n",
    "\n",
    "\n",
    "\n",
    "# Pick some sequences from the validation set\n",
    "num_test_sequences = 50  # Number of sequences to test\n",
    "val_sequences = X_val[:num_test_sequences]\n",
    "val_structures = y_struct_val[:num_test_sequences]\n",
    "\n",
    "# Run predictions and compare with ground truth\n",
    "print(\"Testing the model on some sequences from the validation set:\")\n",
    "with torch.no_grad():\n",
    "    for i, (sequence, true_structure) in enumerate(zip(val_sequences, val_structures)):\n",
    "        sequence = sequence.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        true_structure = true_structure.to(device)\n",
    "        \n",
    "        # Predict structure\n",
    "        pred_structure_logits = model(sequence, true_structure.unsqueeze(0))\n",
    "        pred_structure = torch.argmax(pred_structure_logits.view(-1, 3), dim=-1)\n",
    "        \n",
    "        # Decode sequence and structures\n",
    "        decoded_sequence = ''.join([list('ATCG-')[x.item()] for x in sequence[0]])  # Decode sequence\n",
    "        decoded_true_structure = decode_structure(true_structure)\n",
    "        decoded_pred_structure = decode_structure(pred_structure)\n",
    "\n",
    "        # Enforce symmetry and minimum distance rule\n",
    "        decoded_pred_structure = enforce_symmetry_and_minimum_distance(list(decoded_pred_structure))\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nSequence {i + 1}: {decoded_sequence}\")\n",
    "        print(f\"True Structure:    {decoded_true_structure}\")\n",
    "        print(f\"Predicted Structure: {''.join(decoded_pred_structure)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad42b62-2112-4dbc-babf-7ad0ce1a71bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd878a-7427-4e67-ac28-73ca3c8d160b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
