{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83acf3e0-07d8-4d89-b480-db1c78c056ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of combined dataset after filtering: 1294216\n",
      "Training set size: 1292921\n",
      "Test set size: 1295\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load all datasets\n",
    "with open('database1_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database1_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database2_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database2_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database3_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database3_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database4_1structsonly_34ave.pkl', 'rb') as file:\n",
    "    database4_1structsonly_34ave = pickle.load(file)\n",
    "\n",
    "with open('database5_1structsonly_50ave.pkl', 'rb') as file:\n",
    "    database5_1structsonly_50ave = pickle.load(file)\n",
    "\n",
    "with open('database6_allstructs_50ave.pkl', 'rb') as file:\n",
    "    database6_allstructs_50ave = pickle.load(file)\n",
    "\n",
    "with open('database7_allstructs_50ave.pkl', 'rb') as file:\n",
    "    database7_allstructs_50ave = pickle.load(file)\n",
    "\n",
    "with open('database9_allstructs_40ave.pkl', 'rb') as file:\n",
    "    database9_allstructs_40ave = pickle.load(file)\n",
    "\n",
    "with open('database10_allstructs_40ave.pkl', 'rb') as file:\n",
    "    database10_allstructs_40ave = pickle.load(file)\n",
    "\n",
    "with open('database11_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database11_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database13_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database13_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database14_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database14_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database15_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database15_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database16_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database16_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database17_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database17_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database18_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database18_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database19_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database19_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database20_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database20_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database21_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database21_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database22_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database22_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database23_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database23_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database24_allstructs_45only.pkl', 'rb') as file:\n",
    "    database24_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database25_allstructs_45only.pkl', 'rb') as file:\n",
    "    database25_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database26_allstructs_45only.pkl', 'rb') as file:\n",
    "    database26_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database27_allstructs_45only.pkl', 'rb') as file:\n",
    "    database27_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database28_allstructs_45only.pkl', 'rb') as file:\n",
    "    database28_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database29_allstructs_45only.pkl', 'rb') as file:\n",
    "    database29_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database30_allstructs_45only.pkl', 'rb') as file:\n",
    "    database30_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database31_allstructs_45only.pkl', 'rb') as file:\n",
    "    database31_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database32_allstructs_45only.pkl', 'rb') as file:\n",
    "    database32_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database33_allstructs_45only.pkl', 'rb') as file:\n",
    "    database33_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database34_allstructs_45only.pkl', 'rb') as file:\n",
    "    database34_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database35_allstructs_45only.pkl', 'rb') as file:\n",
    "    database35_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database36_allstructs_45only.pkl', 'rb') as file:\n",
    "    database36_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database37_allstructs_45only.pkl', 'rb') as file:\n",
    "    database37_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database38_allstructs_45only.pkl', 'rb') as file:\n",
    "    database38_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database39_allstructs_45only.pkl', 'rb') as file:\n",
    "    database39_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database40_allstructs_45only.pkl', 'rb') as file:\n",
    "    database40_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database41_allstructs_45only.pkl', 'rb') as file:\n",
    "    database41_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database42_allstructs_45only.pkl', 'rb') as file:\n",
    "    database42_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database43_allstructs_45only.pkl', 'rb') as file:\n",
    "    database43_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database44_allstructs_45only.pkl', 'rb') as file:\n",
    "    database44_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database45_allstructs_45only.pkl', 'rb') as file:\n",
    "    database45_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database46_allstructs_45only.pkl', 'rb') as file:\n",
    "    database46_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database47_allstructs_45only.pkl', 'rb') as file:\n",
    "    database47_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database48_allstructs_45only.pkl', 'rb') as file:\n",
    "    database48_allstructs_45only = pickle.load(file)\n",
    "    \n",
    "\n",
    "# Combine datasets into a single dictionary\n",
    "combined_data = {}\n",
    "datasets = [\n",
    "    database1_allstructs_34ave, database2_allstructs_34ave, database3_allstructs_34ave,\n",
    "    database4_1structsonly_34ave, database5_1structsonly_50ave, database6_allstructs_50ave,\n",
    "    database7_allstructs_50ave, database9_allstructs_40ave, database10_allstructs_40ave, \n",
    "    database11_allstructs_45ave, database13_allstructs_45ave, database14_allstructs_45ave,\n",
    "    database15_allstructs_45ave, database16_allstructs_45ave, database17_allstructs_45ave,\n",
    "    database18_allstructs_45ave, database19_allstructs_45ave, database20_allstructs_45ave,\n",
    "    database21_allstructs_45ave, database22_allstructs_45ave, database23_allstructs_45ave,\n",
    "    database24_allstructs_45only, database25_allstructs_45only, database26_allstructs_45only,\n",
    "    database27_allstructs_45only, database28_allstructs_45only, database29_allstructs_45only,\n",
    "    database30_allstructs_45only, database31_allstructs_45only, database32_allstructs_45only,\n",
    "    database33_allstructs_45only, database34_allstructs_45only, database35_allstructs_45only,\n",
    "    database36_allstructs_45only, database37_allstructs_45only, database38_allstructs_45only,\n",
    "    database39_allstructs_45only, database40_allstructs_45only, database41_allstructs_45only,\n",
    "    database42_allstructs_45only, database43_allstructs_45only, database44_allstructs_45only,\n",
    "    database45_allstructs_45only, database46_allstructs_45only, database47_allstructs_45only,\n",
    "    database48_allstructs_45only\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seq, (temp, structs) in dataset.items():\n",
    "        if temp is not None:\n",
    "            combined_data[seq] = (temp, structs)\n",
    "\n",
    "combined_data2 = {}\n",
    "for i, j in combined_data.items():\n",
    "    if len(i) == len(j[1][-1]):\n",
    "        combined_data2[i] = j\n",
    "\n",
    "combined_data = combined_data2\n",
    "\n",
    "# Verify combined data size after filtering\n",
    "print(f\"Total size of combined dataset after filtering: {len(combined_data)}\")\n",
    "\n",
    "# Prepare data for train-test split\n",
    "sequences = list(combined_data.keys())\n",
    "labels = list(combined_data.values())\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% test)\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Create train and test sets as dictionaries\n",
    "train_set = {seq: label for seq, label in zip(train_sequences, train_labels)}\n",
    "test_set = {seq: label for seq, label in zip(test_sequences, test_labels)}\n",
    "\n",
    "# Display the split summary\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2423e75a-bf43-46fd-b32c-2d0084162646",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_sequences = [seq for seq in train_set.keys() if len(seq) == 45]\n",
    "top_structures = [train_set[seq][1] for seq in dna_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f745fd09-e181-473b-b036-bbd728f49f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777256\n"
     ]
    }
   ],
   "source": [
    "print(len(dna_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef21692e-1c22-4c59-9658-1ba8341a9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TATTTATCAGATCAATGGGGTATGAGCACAGTTAGTGGCCCGGCG',\n",
       " 'GAGCACGTCCGGTGTAAGTGTCTTGCCGGCCAGTACCGAACACGT',\n",
       " 'CTCAGAAATACACTCCTGCCGTGCGGATCAGAGGCACCATATATG',\n",
       " 'CGCTAATTCACAGGTCAACGATATTTAATCGTTTGCGCTGCATTT',\n",
       " 'GAGCCTGGGCGAGGGAGTGGCACATGATTTATCGACCTTAAATTC',\n",
       " 'CGATCGGTACAGCATGGTCGGAGGGCAAGGACAAACAAGTTTCGG',\n",
       " 'CGCTTAGGGCAGTTTAATCTCTGTTGTCCTTATATCGACCATACC',\n",
       " 'GCTACGATACACCCGGGACATGCTTAGATGTCCTATGTGCAACTT',\n",
       " 'TTATTATCCGGTGGGAGTTTATATCTCACTTAATGAGGGGCTCTC',\n",
       " 'TAAGATGCGTGAACAAGAGGGGAATTGGAGCGAAACGGAGCGCTA',\n",
       " 'TGTCCGTAATTCCGCTCCTCTACCTCCCGCCAGGACATACCGAAC',\n",
       " 'CTTTTGAGGGCCAGATGAAGACTCGAGTCCACGCATGTTGGGCCC',\n",
       " 'TGGAGCACGGTTTCTATAGGTTTGAGTCATTCTGCCTTTCAACCG',\n",
       " 'TTGTTATAACTGAAGTATCGTGCCATGTAAACACGCATTTCGGTA',\n",
       " 'TACACCCAAAGCCGCTAATGGATGCAGCTCGCATTGCCTGTCCGA',\n",
       " 'ATTAGGGGCAAGACTAACTGCGGGAGTTAGGGTGAGATCGCGAAA',\n",
       " 'AGGGGCGAATCGATTGGGTTGGGAATTTGTTTCGCGCCAGAGAAC',\n",
       " 'ATTGAACTTTTGTACGCGAATTATCAATGACGGTGGCAGATTTTT',\n",
       " 'CAAATTGGTGTTCCGCGTCAGTGACACGGCCTAATTGTTCAACCC',\n",
       " 'TAGTGCGAGGGTCCACAAAGTCTTACTGTCGTACTGTCCTAGTGG',\n",
       " 'AGTCGCCGCGAATTTAACGTTACCGCGCAGTCGCCAATTAGTAGA',\n",
       " 'CAAAATAACCAGGCCTCACGCTCAATGGGGGCCGCAGAGTGGGCC',\n",
       " 'TTACGACCCAAACCTAACCTGGCGCGGTTGCACCTTAGTGGAAGG',\n",
       " 'TTACAAAATTTACGCGCTGTTGCTTATGGATCGGATTCTGTGAGC',\n",
       " 'TTATAGTTTTGCTGACAGCGGACTGATATCAATACGAATGGTATT',\n",
       " 'AGCTAATACAAATGCGTCCTAATATTCGATTGATGGAAGGTTACC',\n",
       " 'ACCAGCCGCGCGGATCAGGGAACGCACTGTACCATGGGAAACTCT',\n",
       " 'TCTAACGACGAAAGAACCAACTCGTCTACCTCCGTTTATCCGGTG',\n",
       " 'GTAGTCACCACTAACTCTGAGAGGCAGCGTGCGTATGATGTCCGC',\n",
       " 'TGGTAGGGAGGTCCCGACGTTGCGTTTTACCAGGCGTCGGCGCTA']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_sequences[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17c93c17-7328-4b0f-909a-8eed3ffee0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.(((.....))).....((((.....(((.....)))))))....',\n",
       "  '...............((((.(...............).))))...',\n",
       "  '.(((.(.....).)))..........(((.....)))........',\n",
       "  '.(((.........)))..........(((.....)))........',\n",
       "  '.......((......)).........(((.....)))........',\n",
       "  '...........(((........))).(((.....)))........',\n",
       "  '......(((.(((.....))).))).(((.....)))........',\n",
       "  '.................((((.....(((.....)))))))....'],\n",
       " ['........(((((...........))))).....((.......))',\n",
       "  '...(((.....)))...((((.(...(((......))))))))..',\n",
       "  '......(.(((((...........))))))..((.....))....',\n",
       "  '...(((.....)))...((.....))(((......))).......',\n",
       "  '....((..(((((...........)))))...))..((....)).',\n",
       "  '.(((((...........))).))...(((......))).......',\n",
       "  '....((((.((((.....................))))...))))',\n",
       "  '....(((((((((...........)))))............))))',\n",
       "  '...(((.....)))..........(.(((......)))..)....',\n",
       "  '........(((((...........))))).......((....)).',\n",
       "  '...(((.....)))...((((.....(((......))).))))..',\n",
       "  '...(((...........)))......(((......))).......',\n",
       "  '........(((((...........)))))...((.....))....',\n",
       "  '...(((.....))).(((...)))..(((......))).......',\n",
       "  '...(((.....)))............(((......))).......'],\n",
       " ['...((.......))...(((.((.....))..)))..........',\n",
       "  '..........((....))..((((.........))))........',\n",
       "  '.................(((.((.....))..)))..((....))',\n",
       "  '.................(((.((.....))..)))..........',\n",
       "  '..(((..........)))..((((.........))))........'],\n",
       " ['....(((.((...((.((((((.....)))))).))..)).))).',\n",
       "  '(((.(............(((((.....)))))).)))........',\n",
       "  '........((......((((((.....))))))))..........',\n",
       "  '......(.....)...((((((.....)))))).((...))....',\n",
       "  '.((..........)).((((((.....)))))).((...))....',\n",
       "  '.((.............((((((.....))))))......))....',\n",
       "  '........(...)...((((((.....)))))).((...))....',\n",
       "  '..........((....((((((.....))))))))..........',\n",
       "  '....(((...(((...((((((.....))))))....))).))).',\n",
       "  '..((.......))...((((((.....)))))).((...))....',\n",
       "  '..........(((((.((((((.....)))))).)).))).....',\n",
       "  '..........(((..(((((((.....)))).)))..))).....',\n",
       "  '..........(((...((((((.....))))))....))).....',\n",
       "  '................((((((.....)))))).((...))....'],\n",
       " ['...(((.....)))....((...............))........',\n",
       "  '......(.((.........)).)...(((((.......)))))..',\n",
       "  '..((....))........(...)...(((((.......)))))..',\n",
       "  '......((.(((.(...............).))).))........',\n",
       "  '..(((.............)))....((....))............',\n",
       "  '......((.(((..(.(((...)))..)...))).))........',\n",
       "  '...(((.....)))....(...)...(((((.......)))))..',\n",
       "  '......((.(((.....((...)).......))).))........',\n",
       "  '..(((.............))).....(((((.......)))))..',\n",
       "  '......((.(((...................))).))........'],\n",
       " ['..............((.((...)).))...((......)).....',\n",
       "  '....((...((...))..))............(((....)))...',\n",
       "  '......(.....)...(((..........))).............',\n",
       "  '.........((...))(((..........))).............',\n",
       "  '...........((...........))......(((....)))...',\n",
       "  '(((((..........)))))............(((....)))...',\n",
       "  '................(((..........))).............',\n",
       "  '..............((.((...)).)).....(((....)))...'],\n",
       " ['.((.....))............((((.........))))......',\n",
       "  '.....((((((.............))))))...((.....))...',\n",
       "  '.........(((........)))..(((........)))......',\n",
       "  '.((.....))...............(((........)))......',\n",
       "  '.....((((((.............))))))...............'],\n",
       " ['.......((((...(((((((......))))))).))))......',\n",
       "  '((..((.......))((((((......)))))).....)).....',\n",
       "  '...(...).(((..(((((((......)))))))..)))......',\n",
       "  '....((.......))((((((......))))))...((...))..',\n",
       "  '...............((((((......))))))...((...))..',\n",
       "  '.........(((..(((((((......)))))))..)))......'],\n",
       " ['......(((....)))........((((.....)))).(.....)',\n",
       "  '..........((((((.......)))))).....(((.....)))',\n",
       "  '......(((....)))........((((.....))))........'],\n",
       " ['...........((...........))..((((........)))).',\n",
       "  '.....((......)).............((((........)))).',\n",
       "  '.............(((........))).((((........)))).']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_structures[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e9ad8-ec71-4953-a369-33e195ae0407",
   "metadata": {},
   "source": [
    "# Some Parameters Changed and Only Sequences 40-50 bp long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4370469-d5b7-4e42-9991-1450cb0b3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Sample weights stats:\n",
      "Min: 0.1343396008014679, Max: 0.3759748339653015, Mean: 0.22687213122844696\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "sequence_length = 45\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_shared_transformer_blocks = 12\n",
    "num_task_transformer_blocks = 6\n",
    "dropout_rate = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Pair-aware encoding functions\n",
    "def encode_dna_sequence(seq):\n",
    "    encoding = {'A': [1, 0], 'T': [-1, 0], 'C': [0, 1], 'G': [0, -1]}\n",
    "    return [encoding[base] for base in seq]\n",
    "\n",
    "def encode_structure(structure):\n",
    "    encoding = {'.': 0, '(': 1, ')': 2}\n",
    "    return [encoding[char] for char in structure]\n",
    "\n",
    "# Encode DNA sequences\n",
    "dna_sequences = [seq for seq in train_set.keys() if len(seq) == sequence_length]\n",
    "top_structures = [train_set[seq][1] for seq in dna_sequences]\n",
    "\n",
    "# Encode sequences and structures\n",
    "encoded_sequences = [encode_dna_sequence(seq) for seq in dna_sequences]\n",
    "encoded_structures = [encode_structure(structs[-1]) for structs in top_structures]\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(encoded_sequences, dtype=torch.float32)\n",
    "y_struct = torch.tensor(encoded_structures, dtype=torch.long)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_struct_train, y_struct_val = train_test_split(X, y_struct, test_size=0.2, random_state=42)\n",
    "\n",
    "\"\"\"\n",
    "class_counts = torch.tensor(\n",
    "    [sum(struct.count(char) for struct in [\"\".join(top_struct) for top_struct in top_structures]) for char in \".()\"], \n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Ensure no division by zero\n",
    "total_count = class_counts.sum()\n",
    "if total_count == 0 or any(class_counts == 0):\n",
    "    raise ValueError(\"Class counts are invalid, resulting in division by zero.\")\n",
    "\n",
    "# Compute class weights (Inverse frequency weighting)\n",
    "class_weights = total_count / (class_counts + 1e-5)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.clamp(class_weights, min=0)  # Ensure non-negative weights\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Create weighted sampler for the training set\n",
    "sample_weights = []\n",
    "for y in y_struct_train:\n",
    "    valid_indices = y  # Here we are using y directly as it represents the class labels for each base\n",
    "    weight = [class_weights[class_idx].item() for class_idx in valid_indices]\n",
    "    sample_weights.append(sum(weight) / len(weight))  # Average weight for the sequence\n",
    "\n",
    "# Convert to tensor and validate\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "sample_weights = torch.nan_to_num(sample_weights, nan=1.0, posinf=1.0, neginf=1.0)  # Replace NaN/infs with default value\n",
    "sample_weights = torch.clamp(sample_weights, min=0)  # Ensure weights are non-negative\"\"\"\n",
    "\n",
    "class_weights = torch.load(\"class_weights_only45s.pt\").to(device)\n",
    "sample_weights = torch.load(\"sample_weights_only45s.pt\").to(device)\n",
    "\n",
    "# Debugging\n",
    "print(\"Sample weights stats:\")\n",
    "print(f\"Min: {sample_weights.min()}, Max: {sample_weights.max()}, Mean: {sample_weights.mean()}\")\n",
    "assert torch.all(sample_weights >= 0), \"Sample weights contain negative values!\"\n",
    "assert not torch.any(torch.isnan(sample_weights)), \"Sample weights contain NaN values!\"\n",
    "\n",
    "# Create sampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_struct_train)\n",
    "val_dataset = TensorDataset(X_val, y_struct_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f57ca6-0ce2-40d4-80ed-04fb55759afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "class TransformerEncoderBlockWithPairingAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.2, sequence_length=45):\n",
    "        super(TransformerEncoderBlockWithPairingAttention, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.pairing_bias = nn.Parameter(torch.randn(sequence_length, sequence_length))\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        q, k, v = x, x, x\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        seq_len = q.size(1)\n",
    "        pairing_bias_resized = self.pairing_bias[:seq_len, :seq_len]\n",
    "        pairing_bias_resized = pairing_bias_resized.unsqueeze(0).expand(x.size(0), -1, -1).to(x.device)\n",
    "        attn_scores = attn_scores + pairing_bias_resized\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        paired_attn_output = torch.matmul(attn_weights, v)\n",
    "        x = x + self.dropout1(attn_output + paired_attn_output)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.dropout2(self.ffn(x))\n",
    "        x = self.layernorm2(x)\n",
    "        return x\n",
    "\n",
    "class StructurePredictor(nn.Module):\n",
    "    def __init__(self, sequence_length=45, embedding_dim=64, num_heads=8, ff_dim=128, num_shared_blocks=2, num_task_blocks=2):\n",
    "        super(StructurePredictor, self).__init__()\n",
    "        self.dna_projection = nn.Linear(2, embedding_dim)  # Project [1, 0], [-1, 0], etc., to embedding space\n",
    "        self.positional_encoding = self.create_sinusoidal_positional_encoding(sequence_length, embedding_dim)\n",
    "        self.shared_transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlockWithPairingAttention(embedding_dim, num_heads, ff_dim, sequence_length=sequence_length)\n",
    "            for _ in range(num_shared_blocks)\n",
    "        ])\n",
    "        self.struct_transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlockWithPairingAttention(embedding_dim, num_heads, ff_dim, sequence_length=sequence_length)\n",
    "            for _ in range(num_task_blocks)\n",
    "        ])\n",
    "        self.structure_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 3)\n",
    "        )\n",
    "\n",
    "    def create_sinusoidal_positional_encoding(self, seq_len, d_model):\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pos_enc = torch.zeros(seq_len, d_model)\n",
    "        pos_enc[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(pos * div_term)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, encoded_structure=None):\n",
    "        device = x.device\n",
    "        positional_encoding = self.positional_encoding.to(device)\n",
    "    \n",
    "        x = self.dna_projection(x) + positional_encoding\n",
    "        x = x.permute(1, 0, 2)\n",
    "    \n",
    "        # Shared Transformer blocks\n",
    "        for block in self.shared_transformer_blocks:\n",
    "            x = block(x)\n",
    "    \n",
    "        # Output layers\n",
    "        output = self.structure_head(x.permute(1, 0, 2))\n",
    "        return output\n",
    "\n",
    "class RewardedThermodynamicallyBalancedCategoricalCrossEntropy(nn.Module):\n",
    "    def __init__(self, weights, ignore_index=-1, pairing_penalty=0.2, thermo_penalty=0.3, specificity_reward=0.05):\n",
    "        super(RewardedThermodynamicallyBalancedCategoricalCrossEntropy, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.ignore_index = ignore_index\n",
    "        self.pairing_penalty = pairing_penalty\n",
    "        self.thermo_penalty = thermo_penalty\n",
    "        self.specificity_reward = specificity_reward\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mask = (y_true != self.ignore_index)\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "\n",
    "        if y_true.numel() == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True, device=y_pred.device)\n",
    "\n",
    "        # Weighted categorical cross-entropy\n",
    "        y_true_one_hot = F.one_hot(y_true, num_classes=3).float()\n",
    "        log_probs = F.log_softmax(y_pred, dim=-1)\n",
    "        loss = -torch.sum(self.weights * y_true_one_hot * log_probs, dim=-1).mean()\n",
    "\n",
    "        # Pairing imbalance penalty\n",
    "        pred_labels = torch.argmax(y_pred, dim=-1)\n",
    "        open_count = (pred_labels == 1).sum()\n",
    "        close_count = (pred_labels == 2).sum()\n",
    "        imbalance_penalty = self.pairing_penalty * torch.abs(open_count - close_count).float()\n",
    "\n",
    "        # Thermodynamic penalty for mismatches\n",
    "        mismatch_penalty = torch.sum((pred_labels == 1) & (y_true == 2)) * self.thermo_penalty\n",
    "\n",
    "        # Specificity reward for correct pair predictions\n",
    "        correct_pairings = ((pred_labels == y_true) & ((y_true == 1) | (y_true == 2))).sum()\n",
    "        specificity_reward = self.specificity_reward * correct_pairings.float()\n",
    "\n",
    "        return loss + imbalance_penalty + mismatch_penalty - specificity_reward\n",
    "\n",
    "\n",
    "# Instantiate model and loss function\n",
    "model = StructurePredictor(sequence_length, embedding_dim, num_heads, ff_dim, num_shared_transformer_blocks, num_task_transformer_blocks).to(device)\n",
    "loss_fn_struct = RewardedThermodynamicallyBalancedCategoricalCrossEntropy(class_weights)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\n",
    "\n",
    "# Training loop with early stopping and metrics\n",
    "best_val_loss = float('inf')\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "def calculate_class_metrics(y_true, y_pred, num_classes=3):\n",
    "    metrics = {}\n",
    "    for class_idx in range(num_classes):\n",
    "        tp = ((y_pred == class_idx) & (y_true == class_idx)).sum().item()\n",
    "        fp = ((y_pred == class_idx) & (y_true != class_idx)).sum().item()\n",
    "        fn = ((y_pred != class_idx) & (y_true == class_idx)).sum().item()\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        metrics[class_idx] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    return metrics\n",
    "\n",
    "def print_validation_metrics(val_metrics, printer=True):\n",
    "    total_f1 = 0\n",
    "    num_classes = len(val_metrics)\n",
    "    \n",
    "    # Print metrics for each class\n",
    "    for cls, metrics in val_metrics.items():\n",
    "        if printer:\n",
    "            print(f\"Class {cls} - Precision: {metrics['precision']:.4f}, \"\n",
    "                  f\"Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
    "        total_f1 += metrics['f1']\n",
    "    \n",
    "    # Calculate the average F1 score\n",
    "    avg_f1 = total_f1 / num_classes if num_classes > 0 else 0.0\n",
    "    \n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cad421-5c8e-4439-b0c1-a8ac50b5800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Save sample weights\\ntorch.save(sample_weights, \"sample_weights_evenevenmoredata.pt\")\\n\\n# Load sample weights\\ntorch.save(class_weights, \"class_weights_evenevenmoredata.pt\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Save sample weights\n",
    "torch.save(sample_weights, \"sample_weights_evenevenmoredata.pt\")\n",
    "\n",
    "# Load sample weights\n",
    "torch.save(class_weights, \"class_weights_evenevenmoredata.pt\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b1c9865-6de3-4c08-be14-0b42aa449558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\\nmodel_path = \"12-2_integerenc.pth\"\\nmodel.load_state_dict(torch.load(model_path, map_location=device))'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run this cell if the gradients explode!\n",
    "\"\"\"\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\n",
    "model_path = \"12-2_integerenc.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2ca60-346a-4489-92cf-57934063aa03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 6.5313\n",
      "Learning Rate: 8.131564889193507e-06\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0031\n",
      "shared_transformer_blocks: 0.0048\n",
      "structure_head: 0.0265\n",
      "Class 0 - Precision: 0.8304, Recall: 0.3475, F1-Score: 0.4899\n",
      "Class 1 - Precision: 0.3010, Recall: 0.6551, F1-Score: 0.4125\n",
      "Class 2 - Precision: 0.3036, Recall: 0.6519, F1-Score: 0.4142\n",
      "Validation Loss: 4.5092, Accuracy: 0.4529, F1: 0.4389\n",
      "Best model saved with validation loss: 4.5092\n",
      "\n",
      "\n",
      "Epoch 2, Training Loss: 5.0387\n",
      "Learning Rate: 8.52589894593923e-06\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0036\n",
      "shared_transformer_blocks: 0.0052\n",
      "structure_head: 0.0272\n",
      "Class 0 - Precision: 0.8565, Recall: 0.3255, F1-Score: 0.4717\n",
      "Class 1 - Precision: 0.3137, Recall: 0.6841, F1-Score: 0.4301\n",
      "Class 2 - Precision: 0.3079, Recall: 0.7071, F1-Score: 0.4290\n",
      "Validation Loss: 4.3436, Accuracy: 0.4668, F1: 0.4436\n",
      "Best model saved with validation loss: 4.3436\n",
      "\n",
      "\n",
      "Epoch 3, Training Loss: 2.3734\n",
      "Learning Rate: 9.181921326143828e-06\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0058\n",
      "shared_transformer_blocks: 0.0075\n",
      "structure_head: 0.0298\n",
      "Class 0 - Precision: 0.8746, Recall: 0.3859, F1-Score: 0.5355\n",
      "Class 1 - Precision: 0.3402, Recall: 0.7221, F1-Score: 0.4625\n",
      "Class 2 - Precision: 0.3401, Recall: 0.7178, F1-Score: 0.4615\n",
      "Validation Loss: -0.3913, Accuracy: 0.4996, F1: 0.4865\n",
      "Best model saved with validation loss: -0.3913\n",
      "\n",
      "\n",
      "Epoch 4, Training Loss: -0.2605\n",
      "Learning Rate: 1.009783391497899e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0070\n",
      "shared_transformer_blocks: 0.0090\n",
      "structure_head: 0.0297\n",
      "Class 0 - Precision: 0.8795, Recall: 0.4143, F1-Score: 0.5633\n",
      "Class 1 - Precision: 0.3549, Recall: 0.7412, F1-Score: 0.4800\n",
      "Class 2 - Precision: 0.3576, Recall: 0.7243, F1-Score: 0.4788\n",
      "Validation Loss: -0.9704, Accuracy: 0.5225, F1: 0.5074\n",
      "Best model saved with validation loss: -0.9704\n",
      "\n",
      "\n",
      "Epoch 5, Training Loss: -1.3628\n",
      "Learning Rate: 1.1271126255397556e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0073\n",
      "shared_transformer_blocks: 0.0096\n",
      "structure_head: 0.0290\n",
      "Class 0 - Precision: 0.8882, Recall: 0.4158, F1-Score: 0.5664\n",
      "Class 1 - Precision: 0.3655, Recall: 0.7391, F1-Score: 0.4891\n",
      "Class 2 - Precision: 0.3606, Recall: 0.7583, F1-Score: 0.4888\n",
      "Validation Loss: -2.8830, Accuracy: 0.5324, F1: 0.5148\n",
      "Best model saved with validation loss: -2.8830\n",
      "\n",
      "\n",
      "Epoch 6, Training Loss: -2.2899\n",
      "Learning Rate: 1.2698582429133791e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0075\n",
      "shared_transformer_blocks: 0.0100\n",
      "structure_head: 0.0284\n",
      "Class 0 - Precision: 0.8973, Recall: 0.4203, F1-Score: 0.5724\n",
      "Class 1 - Precision: 0.3737, Recall: 0.7616, F1-Score: 0.5014\n",
      "Class 2 - Precision: 0.3697, Recall: 0.7713, F1-Score: 0.4999\n",
      "Validation Loss: -4.2661, Accuracy: 0.5420, F1: 0.5246\n",
      "Best model saved with validation loss: -4.2661\n",
      "\n",
      "\n",
      "Epoch 7, Training Loss: -3.1810\n",
      "Learning Rate: 1.4376289871326675e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0080\n",
      "shared_transformer_blocks: 0.0107\n",
      "structure_head: 0.0279\n",
      "Class 0 - Precision: 0.9053, Recall: 0.4318, F1-Score: 0.5847\n",
      "Class 1 - Precision: 0.3856, Recall: 0.7670, F1-Score: 0.5132\n",
      "Class 2 - Precision: 0.3782, Recall: 0.7945, F1-Score: 0.5125\n",
      "Validation Loss: -4.3118, Accuracy: 0.5538, F1: 0.5368\n",
      "Best model saved with validation loss: -4.3118\n",
      "\n",
      "\n",
      "Epoch 8, Training Loss: -3.8922\n",
      "Learning Rate: 1.6299650094606864e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0085\n",
      "shared_transformer_blocks: 0.0115\n",
      "structure_head: 0.0274\n",
      "Class 0 - Precision: 0.9110, Recall: 0.4551, F1-Score: 0.6069\n",
      "Class 1 - Precision: 0.3978, Recall: 0.7805, F1-Score: 0.5270\n",
      "Class 2 - Precision: 0.3939, Recall: 0.8028, F1-Score: 0.5285\n",
      "Validation Loss: -6.3714, Accuracy: 0.5677, F1: 0.5542\n",
      "Best model saved with validation loss: -6.3714\n",
      "\n",
      "\n",
      "Epoch 9, Training Loss: -5.7889\n",
      "Learning Rate: 1.8463391293252333e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0085\n",
      "shared_transformer_blocks: 0.0122\n",
      "structure_head: 0.0266\n",
      "Class 0 - Precision: 0.9315, Recall: 0.4842, F1-Score: 0.6372\n",
      "Class 1 - Precision: 0.4323, Recall: 0.8280, F1-Score: 0.5680\n",
      "Class 2 - Precision: 0.4228, Recall: 0.8470, F1-Score: 0.5641\n",
      "Validation Loss: -8.8793, Accuracy: 0.5962, F1: 0.5898\n",
      "Best model saved with validation loss: -8.8793\n",
      "\n",
      "\n",
      "Epoch 10, Training Loss: -8.2658\n",
      "Learning Rate: 2.0861582792866323e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0064\n",
      "shared_transformer_blocks: 0.0112\n",
      "structure_head: 0.0254\n",
      "Class 0 - Precision: 0.9298, Recall: 0.5365, F1-Score: 0.6804\n",
      "Class 1 - Precision: 0.4549, Recall: 0.8309, F1-Score: 0.5879\n",
      "Class 2 - Precision: 0.4507, Recall: 0.8399, F1-Score: 0.5866\n",
      "Validation Loss: -10.6554, Accuracy: 0.6352, F1: 0.6183\n",
      "Best model saved with validation loss: -10.6554\n",
      "\n",
      "\n",
      "Epoch 11, Training Loss: -9.1969\n",
      "Learning Rate: 2.348765130597157e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0048\n",
      "shared_transformer_blocks: 0.0101\n",
      "structure_head: 0.0249\n",
      "Class 0 - Precision: 0.9316, Recall: 0.5543, F1-Score: 0.6951\n",
      "Class 1 - Precision: 0.4679, Recall: 0.8346, F1-Score: 0.5996\n",
      "Class 2 - Precision: 0.4592, Recall: 0.8424, F1-Score: 0.5944\n",
      "Validation Loss: -10.5135, Accuracy: 0.6523, F1: 0.6297\n",
      "\n",
      "\n",
      "Epoch 12, Training Loss: -9.8638\n",
      "Learning Rate: 2.63343989489651e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0037\n",
      "shared_transformer_blocks: 0.0094\n",
      "structure_head: 0.0246\n",
      "Class 0 - Precision: 0.9353, Recall: 0.5632, F1-Score: 0.7031\n",
      "Class 1 - Precision: 0.4741, Recall: 0.8421, F1-Score: 0.6067\n",
      "Class 2 - Precision: 0.4677, Recall: 0.8483, F1-Score: 0.6030\n",
      "Validation Loss: -11.2179, Accuracy: 0.6633, F1: 0.6376\n",
      "Best model saved with validation loss: -11.2179\n",
      "\n",
      "\n",
      "Epoch 13, Training Loss: -10.3515\n",
      "Learning Rate: 2.939402297105084e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0032\n",
      "shared_transformer_blocks: 0.0092\n",
      "structure_head: 0.0243\n",
      "Class 0 - Precision: 0.9416, Recall: 0.5686, F1-Score: 0.7091\n",
      "Class 1 - Precision: 0.4839, Recall: 0.8480, F1-Score: 0.6162\n",
      "Class 2 - Precision: 0.4725, Recall: 0.8649, F1-Score: 0.6111\n",
      "Validation Loss: -11.2256, Accuracy: 0.6730, F1: 0.6455\n",
      "Best model saved with validation loss: -11.2256\n",
      "\n",
      "\n",
      "Epoch 14, Training Loss: -10.8921\n",
      "Learning Rate: 3.265813714107378e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0029\n",
      "shared_transformer_blocks: 0.0093\n",
      "structure_head: 0.0238\n",
      "Class 0 - Precision: 0.9409, Recall: 0.6008, F1-Score: 0.7333\n",
      "Class 1 - Precision: 0.4983, Recall: 0.8591, F1-Score: 0.6307\n",
      "Class 2 - Precision: 0.4974, Recall: 0.8556, F1-Score: 0.6290\n",
      "Validation Loss: -12.4669, Accuracy: 0.6862, F1: 0.6644\n",
      "Best model saved with validation loss: -12.4669\n",
      "\n",
      "\n",
      "Epoch 15, Training Loss: -11.4133\n",
      "Learning Rate: 3.6117794733636166e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0028\n",
      "shared_transformer_blocks: 0.0095\n",
      "structure_head: 0.0236\n",
      "Class 0 - Precision: 0.9410, Recall: 0.6381, F1-Score: 0.7605\n",
      "Class 1 - Precision: 0.5272, Recall: 0.8537, F1-Score: 0.6519\n",
      "Class 2 - Precision: 0.5159, Recall: 0.8601, F1-Score: 0.6450\n",
      "Validation Loss: -12.5326, Accuracy: 0.7002, F1: 0.6858\n",
      "Best model saved with validation loss: -12.5326\n",
      "\n",
      "\n",
      "Epoch 16, Training Loss: -11.9110\n",
      "Learning Rate: 3.976351305149184e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0026\n",
      "shared_transformer_blocks: 0.0096\n",
      "structure_head: 0.0233\n",
      "Class 0 - Precision: 0.9442, Recall: 0.6403, F1-Score: 0.7631\n",
      "Class 1 - Precision: 0.5269, Recall: 0.8616, F1-Score: 0.6539\n",
      "Class 2 - Precision: 0.5253, Recall: 0.8672, F1-Score: 0.6543\n",
      "Validation Loss: -13.8279, Accuracy: 0.7094, F1: 0.6904\n",
      "Best model saved with validation loss: -13.8279\n",
      "\n",
      "\n",
      "Epoch 17, Training Loss: -12.3333\n",
      "Learning Rate: 4.358529941700506e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0025\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0227\n",
      "Class 0 - Precision: 0.9478, Recall: 0.6542, F1-Score: 0.7741\n",
      "Class 1 - Precision: 0.5401, Recall: 0.8691, F1-Score: 0.6662\n",
      "Class 2 - Precision: 0.5358, Recall: 0.8726, F1-Score: 0.6639\n",
      "Validation Loss: -14.0116, Accuracy: 0.7191, F1: 0.7014\n",
      "Best model saved with validation loss: -14.0116\n",
      "\n",
      "\n",
      "Epoch 18, Training Loss: -12.7321\n",
      "Learning Rate: 4.757267856143199e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0024\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0221\n",
      "Class 0 - Precision: 0.9479, Recall: 0.6697, F1-Score: 0.7849\n",
      "Class 1 - Precision: 0.5544, Recall: 0.8719, F1-Score: 0.6778\n",
      "Class 2 - Precision: 0.5454, Recall: 0.8726, F1-Score: 0.6712\n",
      "Validation Loss: -13.9629, Accuracy: 0.7280, F1: 0.7113\n",
      "\n",
      "\n",
      "Epoch 19, Training Loss: -13.1047\n",
      "Learning Rate: 5.171472133695375e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0024\n",
      "shared_transformer_blocks: 0.0098\n",
      "structure_head: 0.0218\n",
      "Class 0 - Precision: 0.9522, Recall: 0.6745, F1-Score: 0.7897\n",
      "Class 1 - Precision: 0.5582, Recall: 0.8777, F1-Score: 0.6824\n",
      "Class 2 - Precision: 0.5528, Recall: 0.8805, F1-Score: 0.6792\n",
      "Validation Loss: -14.4025, Accuracy: 0.7353, F1: 0.7171\n",
      "Best model saved with validation loss: -14.4025\n",
      "\n",
      "\n",
      "Epoch 20, Training Loss: -13.4513\n",
      "Learning Rate: 5.6000074672762066e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0024\n",
      "shared_transformer_blocks: 0.0098\n",
      "structure_head: 0.0213\n",
      "Class 0 - Precision: 0.9522, Recall: 0.6862, F1-Score: 0.7976\n",
      "Class 1 - Precision: 0.5651, Recall: 0.8805, F1-Score: 0.6884\n",
      "Class 2 - Precision: 0.5642, Recall: 0.8789, F1-Score: 0.6872\n",
      "Validation Loss: -14.7528, Accuracy: 0.7428, F1: 0.7244\n",
      "Best model saved with validation loss: -14.7528\n",
      "\n",
      "\n",
      "Epoch 21, Training Loss: -13.6933\n",
      "Learning Rate: 6.0416992693090665e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0023\n",
      "shared_transformer_blocks: 0.0098\n",
      "structure_head: 0.0209\n",
      "Class 0 - Precision: 0.9476, Recall: 0.7174, F1-Score: 0.8166\n",
      "Class 1 - Precision: 0.5892, Recall: 0.8695, F1-Score: 0.7025\n",
      "Class 2 - Precision: 0.5843, Recall: 0.8734, F1-Score: 0.7002\n",
      "Validation Loss: -14.9221, Accuracy: 0.7514, F1: 0.7397\n",
      "Best model saved with validation loss: -14.9221\n",
      "\n",
      "\n",
      "Epoch 22, Training Loss: -13.9475\n",
      "Learning Rate: 6.495336891189954e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0022\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0204\n",
      "Class 0 - Precision: 0.9541, Recall: 0.7087, F1-Score: 0.8133\n",
      "Class 1 - Precision: 0.5823, Recall: 0.8883, F1-Score: 0.7034\n",
      "Class 2 - Precision: 0.5846, Recall: 0.8782, F1-Score: 0.7020\n",
      "Validation Loss: -14.5326, Accuracy: 0.7553, F1: 0.7396\n",
      "\n",
      "\n",
      "Epoch 23, Training Loss: -14.1362\n",
      "Learning Rate: 6.95967694159685e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0022\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0200\n",
      "Class 0 - Precision: 0.9469, Recall: 0.7428, F1-Score: 0.8326\n",
      "Class 1 - Precision: 0.6143, Recall: 0.8659, F1-Score: 0.7187\n",
      "Class 2 - Precision: 0.6015, Recall: 0.8733, F1-Score: 0.7123\n",
      "Validation Loss: -14.4839, Accuracy: 0.7627, F1: 0.7545\n",
      "\n",
      "\n",
      "Epoch 24, Training Loss: -14.3328\n",
      "Learning Rate: 7.433446694544762e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0022\n",
      "shared_transformer_blocks: 0.0096\n",
      "structure_head: 0.0196\n",
      "Class 0 - Precision: 0.9536, Recall: 0.7280, F1-Score: 0.8257\n",
      "Class 1 - Precision: 0.6016, Recall: 0.8833, F1-Score: 0.7157\n",
      "Class 2 - Precision: 0.5978, Recall: 0.8830, F1-Score: 0.7129\n",
      "Validation Loss: -15.3733, Accuracy: 0.7650, F1: 0.7514\n",
      "Best model saved with validation loss: -15.3733\n",
      "\n",
      "\n",
      "Epoch 25, Training Loss: -14.5158\n",
      "Learning Rate: 7.91534757784515e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0022\n",
      "shared_transformer_blocks: 0.0095\n",
      "structure_head: 0.0190\n",
      "Class 0 - Precision: 0.9523, Recall: 0.7370, F1-Score: 0.8309\n",
      "Class 1 - Precision: 0.6098, Recall: 0.8781, F1-Score: 0.7197\n",
      "Class 2 - Precision: 0.6030, Recall: 0.8825, F1-Score: 0.7165\n",
      "Validation Loss: -15.4041, Accuracy: 0.7689, F1: 0.7557\n",
      "Best model saved with validation loss: -15.4041\n",
      "\n",
      "\n",
      "Epoch 26, Training Loss: -14.6345\n",
      "Learning Rate: 8.404058732408145e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0021\n",
      "shared_transformer_blocks: 0.0095\n",
      "structure_head: 0.0186\n",
      "Class 0 - Precision: 0.9538, Recall: 0.7324, F1-Score: 0.8286\n",
      "Class 1 - Precision: 0.6103, Recall: 0.8823, F1-Score: 0.7215\n",
      "Class 2 - Precision: 0.5984, Recall: 0.8869, F1-Score: 0.7146\n",
      "Validation Loss: -15.0942, Accuracy: 0.7709, F1: 0.7549\n",
      "\n",
      "\n",
      "Epoch 27, Training Loss: -14.7787\n",
      "Learning Rate: 8.898240632631713e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0020\n",
      "shared_transformer_blocks: 0.0093\n",
      "structure_head: 0.0180\n",
      "Class 0 - Precision: 0.9574, Recall: 0.7224, F1-Score: 0.8235\n",
      "Class 1 - Precision: 0.6016, Recall: 0.8893, F1-Score: 0.7177\n",
      "Class 2 - Precision: 0.5930, Recall: 0.8908, F1-Score: 0.7120\n",
      "Validation Loss: -15.2929, Accuracy: 0.7728, F1: 0.7511\n",
      "\n",
      "\n",
      "Epoch 28, Training Loss: -14.8820\n",
      "Learning Rate: 9.396538757954509e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0021\n",
      "shared_transformer_blocks: 0.0093\n",
      "structure_head: 0.0175\n",
      "Class 0 - Precision: 0.9596, Recall: 0.7190, F1-Score: 0.8221\n",
      "Class 1 - Precision: 0.6037, Recall: 0.8904, F1-Score: 0.7196\n",
      "Class 2 - Precision: 0.5896, Recall: 0.9005, F1-Score: 0.7126\n",
      "Validation Loss: -15.2307, Accuracy: 0.7743, F1: 0.7514\n",
      "\n",
      "\n",
      "Epoch 29, Training Loss: -15.0119\n",
      "Learning Rate: 9.897587305508968e-05\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0020\n",
      "shared_transformer_blocks: 0.0092\n",
      "structure_head: 0.0169\n",
      "Class 0 - Precision: 0.9480, Recall: 0.7703, F1-Score: 0.8500\n",
      "Class 1 - Precision: 0.6389, Recall: 0.8686, F1-Score: 0.7362\n",
      "Class 2 - Precision: 0.6293, Recall: 0.8743, F1-Score: 0.7318\n",
      "Validation Loss: -15.2677, Accuracy: 0.7815, F1: 0.7727\n",
      "\n",
      "\n",
      "Epoch 30, Training Loss: -15.0734\n",
      "Learning Rate: 0.00010400012933698426\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0021\n",
      "shared_transformer_blocks: 0.0092\n",
      "structure_head: 0.0161\n",
      "Class 0 - Precision: 0.9585, Recall: 0.7399, F1-Score: 0.8352\n",
      "Class 1 - Precision: 0.6223, Recall: 0.8893, F1-Score: 0.7322\n",
      "Class 2 - Precision: 0.6039, Recall: 0.8953, F1-Score: 0.7213\n",
      "Validation Loss: -15.0240, Accuracy: 0.7806, F1: 0.7629\n",
      "\n",
      "\n",
      "Epoch 31, Training Loss: -15.2324\n",
      "Learning Rate: 0.00010902438526437461\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0020\n",
      "shared_transformer_blocks: 0.0092\n",
      "structure_head: 0.0156\n",
      "Class 0 - Precision: 0.9666, Recall: 0.7044, F1-Score: 0.8149\n",
      "Class 1 - Precision: 0.5887, Recall: 0.9078, F1-Score: 0.7143\n",
      "Class 2 - Precision: 0.5881, Recall: 0.9068, F1-Score: 0.7134\n",
      "Validation Loss: -15.8001, Accuracy: 0.7789, F1: 0.7475\n",
      "Best model saved with validation loss: -15.8001\n",
      "\n",
      "\n",
      "Epoch 32, Training Loss: -15.3523\n",
      "Learning Rate: 0.00011403486967737796\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0020\n",
      "shared_transformer_blocks: 0.0093\n",
      "structure_head: 0.0150\n",
      "Class 0 - Precision: 0.9573, Recall: 0.7529, F1-Score: 0.8429\n",
      "Class 1 - Precision: 0.6284, Recall: 0.8909, F1-Score: 0.7370\n",
      "Class 2 - Precision: 0.6219, Recall: 0.8928, F1-Score: 0.7331\n",
      "Validation Loss: -15.8790, Accuracy: 0.7861, F1: 0.7710\n",
      "Best model saved with validation loss: -15.8790\n",
      "\n",
      "\n",
      "Epoch 33, Training Loss: -15.4754\n",
      "Learning Rate: 0.00011901784916294019\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0021\n",
      "shared_transformer_blocks: 0.0094\n",
      "structure_head: 0.0142\n",
      "Class 0 - Precision: 0.9626, Recall: 0.7418, F1-Score: 0.8379\n",
      "Class 1 - Precision: 0.6249, Recall: 0.9003, F1-Score: 0.7378\n",
      "Class 2 - Precision: 0.6120, Recall: 0.9032, F1-Score: 0.7296\n",
      "Validation Loss: -15.8358, Accuracy: 0.7876, F1: 0.7684\n",
      "\n",
      "\n",
      "Epoch 34, Training Loss: -15.5621\n",
      "Learning Rate: 0.0001239596656972306\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0021\n",
      "shared_transformer_blocks: 0.0095\n",
      "structure_head: 0.0136\n",
      "Class 0 - Precision: 0.9612, Recall: 0.7550, F1-Score: 0.8457\n",
      "Class 1 - Precision: 0.6353, Recall: 0.8947, F1-Score: 0.7430\n",
      "Class 2 - Precision: 0.6232, Recall: 0.9030, F1-Score: 0.7374\n",
      "Validation Loss: -15.9579, Accuracy: 0.7915, F1: 0.7754\n",
      "Best model saved with validation loss: -15.9579\n",
      "\n",
      "\n",
      "Epoch 35, Training Loss: -15.6801\n",
      "Learning Rate: 0.00012884677408140028\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0022\n",
      "shared_transformer_blocks: 0.0095\n",
      "structure_head: 0.0131\n",
      "Class 0 - Precision: 0.9514, Recall: 0.7993, F1-Score: 0.8687\n",
      "Class 1 - Precision: 0.6718, Recall: 0.8759, F1-Score: 0.7604\n",
      "Class 2 - Precision: 0.6622, Recall: 0.8841, F1-Score: 0.7572\n",
      "Validation Loss: -16.0304, Accuracy: 0.7980, F1: 0.7954\n",
      "Best model saved with validation loss: -16.0304\n",
      "\n",
      "\n",
      "Epoch 36, Training Loss: -15.8013\n",
      "Learning Rate: 0.0001336657790680942\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0022\n",
      "shared_transformer_blocks: 0.0096\n",
      "structure_head: 0.0124\n",
      "Class 0 - Precision: 0.9593, Recall: 0.7719, F1-Score: 0.8554\n",
      "Class 1 - Precision: 0.6471, Recall: 0.8955, F1-Score: 0.7513\n",
      "Class 2 - Precision: 0.6413, Recall: 0.8957, F1-Score: 0.7474\n",
      "Validation Loss: -16.2919, Accuracy: 0.7966, F1: 0.7847\n",
      "Best model saved with validation loss: -16.2919\n",
      "\n",
      "\n",
      "Epoch 37, Training Loss: -15.8920\n",
      "Learning Rate: 0.00013840347207695622\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0023\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0119\n",
      "Class 0 - Precision: 0.9565, Recall: 0.7883, F1-Score: 0.8643\n",
      "Class 1 - Precision: 0.6627, Recall: 0.8886, F1-Score: 0.7592\n",
      "Class 2 - Precision: 0.6554, Recall: 0.8927, F1-Score: 0.7558\n",
      "Validation Loss: -16.3694, Accuracy: 0.8002, F1: 0.7931\n",
      "Best model saved with validation loss: -16.3694\n",
      "\n",
      "\n",
      "Epoch 38, Training Loss: -16.0018\n",
      "Learning Rate: 0.00014304686739849165\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0023\n",
      "shared_transformer_blocks: 0.0096\n",
      "structure_head: 0.0115\n",
      "Class 0 - Precision: 0.9622, Recall: 0.7682, F1-Score: 0.8543\n",
      "Class 1 - Precision: 0.6506, Recall: 0.8984, F1-Score: 0.7547\n",
      "Class 2 - Precision: 0.6355, Recall: 0.9054, F1-Score: 0.7468\n",
      "Validation Loss: -16.0420, Accuracy: 0.7998, F1: 0.7853\n",
      "\n",
      "\n",
      "Epoch 39, Training Loss: -16.0460\n",
      "Learning Rate: 0.0001475832377870555\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0024\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0111\n",
      "Class 0 - Precision: 0.9632, Recall: 0.7692, F1-Score: 0.8553\n",
      "Class 1 - Precision: 0.6502, Recall: 0.9035, F1-Score: 0.7562\n",
      "Class 2 - Precision: 0.6391, Recall: 0.9045, F1-Score: 0.7490\n",
      "Validation Loss: -16.3369, Accuracy: 0.8014, F1: 0.7868\n",
      "\n",
      "\n",
      "Epoch 40, Training Loss: -16.1662\n",
      "Learning Rate: 0.00015200014934540793\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0024\n",
      "shared_transformer_blocks: 0.0097\n",
      "structure_head: 0.0107\n",
      "Class 0 - Precision: 0.9645, Recall: 0.7692, F1-Score: 0.8559\n",
      "Class 1 - Precision: 0.6511, Recall: 0.9020, F1-Score: 0.7563\n",
      "Class 2 - Precision: 0.6399, Recall: 0.9110, F1-Score: 0.7518\n",
      "Validation Loss: -16.5505, Accuracy: 0.8024, F1: 0.7880\n",
      "Best model saved with validation loss: -16.5505\n",
      "\n",
      "\n",
      "Epoch 41, Training Loss: -16.1798\n",
      "Learning Rate: 0.0001562854956052208\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0025\n",
      "shared_transformer_blocks: 0.0098\n",
      "structure_head: 0.0104\n",
      "Class 0 - Precision: 0.9633, Recall: 0.7732, F1-Score: 0.8579\n",
      "Class 1 - Precision: 0.6557, Recall: 0.8998, F1-Score: 0.7586\n",
      "Class 2 - Precision: 0.6418, Recall: 0.9089, F1-Score: 0.7523\n",
      "Validation Loss: -16.3382, Accuracy: 0.8037, F1: 0.7896\n",
      "\n",
      "\n",
      "Epoch 42, Training Loss: -16.2255\n",
      "Learning Rate: 0.00016042753071012265\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0027\n",
      "shared_transformer_blocks: 0.0098\n",
      "structure_head: 0.0100\n",
      "Class 0 - Precision: 0.9616, Recall: 0.7784, F1-Score: 0.8603\n",
      "Class 1 - Precision: 0.6564, Recall: 0.8958, F1-Score: 0.7577\n",
      "Class 2 - Precision: 0.6475, Recall: 0.9043, F1-Score: 0.7547\n",
      "Validation Loss: -16.6251, Accuracy: 0.8050, F1: 0.7909\n",
      "Best model saved with validation loss: -16.6251\n",
      "\n",
      "\n",
      "Epoch 43, Training Loss: -16.3402\n",
      "Learning Rate: 0.0001644149016103299\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0026\n",
      "shared_transformer_blocks: 0.0098\n",
      "structure_head: 0.0098\n",
      "Class 0 - Precision: 0.9651, Recall: 0.7736, F1-Score: 0.8588\n",
      "Class 1 - Precision: 0.6521, Recall: 0.9070, F1-Score: 0.7587\n",
      "Class 2 - Precision: 0.6487, Recall: 0.9094, F1-Score: 0.7572\n",
      "Validation Loss: -17.0279, Accuracy: 0.8063, F1: 0.7916\n",
      "Best model saved with validation loss: -17.0279\n",
      "\n",
      "\n",
      "Epoch 44, Training Loss: -16.3840\n",
      "Learning Rate: 0.00016823667918062054\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0028\n",
      "shared_transformer_blocks: 0.0099\n",
      "structure_head: 0.0095\n",
      "Class 0 - Precision: 0.9633, Recall: 0.7833, F1-Score: 0.8640\n",
      "Class 1 - Precision: 0.6583, Recall: 0.9025, F1-Score: 0.7613\n",
      "Class 2 - Precision: 0.6589, Recall: 0.9065, F1-Score: 0.7631\n",
      "Validation Loss: -17.1603, Accuracy: 0.8082, F1: 0.7962\n",
      "Best model saved with validation loss: -17.1603\n",
      "\n",
      "\n",
      "Epoch 45, Training Loss: -16.4842\n",
      "Learning Rate: 0.0001718823881763579\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0028\n",
      "shared_transformer_blocks: 0.0100\n",
      "structure_head: 0.0093\n",
      "Class 0 - Precision: 0.9667, Recall: 0.7699, F1-Score: 0.8572\n",
      "Class 1 - Precision: 0.6481, Recall: 0.9114, F1-Score: 0.7576\n",
      "Class 2 - Precision: 0.6467, Recall: 0.9101, F1-Score: 0.7561\n",
      "Validation Loss: -16.9135, Accuracy: 0.8081, F1: 0.7903\n",
      "\n",
      "\n",
      "Epoch 46, Training Loss: -16.4863\n",
      "Learning Rate: 0.0001753420359454575\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0029\n",
      "shared_transformer_blocks: 0.0100\n",
      "structure_head: 0.0092\n",
      "Class 0 - Precision: 0.9639, Recall: 0.7835, F1-Score: 0.8644\n",
      "Class 1 - Precision: 0.6615, Recall: 0.9067, F1-Score: 0.7650\n",
      "Class 2 - Precision: 0.6577, Recall: 0.9057, F1-Score: 0.7621\n",
      "Validation Loss: -16.9588, Accuracy: 0.8103, F1: 0.7971\n",
      "\n",
      "\n",
      "Epoch 47, Training Loss: -16.5329\n",
      "Learning Rate: 0.00017860613981759873\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0030\n",
      "shared_transformer_blocks: 0.0101\n",
      "structure_head: 0.0090\n",
      "Class 0 - Precision: 0.9583, Recall: 0.8099, F1-Score: 0.8779\n",
      "Class 1 - Precision: 0.6886, Recall: 0.8906, F1-Score: 0.7767\n",
      "Class 2 - Precision: 0.6771, Recall: 0.8977, F1-Score: 0.7720\n",
      "Validation Loss: -16.6601, Accuracy: 0.8136, F1: 0.8088\n",
      "\n",
      "\n",
      "Epoch 48, Training Loss: -16.6145\n",
      "Learning Rate: 0.0001816657530956096\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0030\n",
      "shared_transformer_blocks: 0.0100\n",
      "structure_head: 0.0088\n",
      "Class 0 - Precision: 0.9627, Recall: 0.7966, F1-Score: 0.8718\n",
      "Class 1 - Precision: 0.6759, Recall: 0.9004, F1-Score: 0.7721\n",
      "Class 2 - Precision: 0.6673, Recall: 0.9057, F1-Score: 0.7684\n",
      "Validation Loss: -17.0166, Accuracy: 0.8134, F1: 0.8041\n",
      "\n",
      "\n",
      "Epoch 49, Training Loss: -16.6310\n",
      "Learning Rate: 0.00018451248957778395\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0030\n",
      "shared_transformer_blocks: 0.0100\n",
      "structure_head: 0.0087\n",
      "Class 0 - Precision: 0.9646, Recall: 0.7877, F1-Score: 0.8672\n",
      "Class 1 - Precision: 0.6695, Recall: 0.9041, F1-Score: 0.7693\n",
      "Class 2 - Precision: 0.6582, Recall: 0.9100, F1-Score: 0.7639\n",
      "Validation Loss: -16.8708, Accuracy: 0.8132, F1: 0.8001\n",
      "\n",
      "\n",
      "Epoch 50, Training Loss: -16.7176\n",
      "Learning Rate: 0.00018713854654391676\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0030\n",
      "shared_transformer_blocks: 0.0100\n",
      "structure_head: 0.0086\n",
      "Class 0 - Precision: 0.9661, Recall: 0.7920, F1-Score: 0.8704\n",
      "Class 1 - Precision: 0.6741, Recall: 0.9077, F1-Score: 0.7736\n",
      "Class 2 - Precision: 0.6651, Recall: 0.9134, F1-Score: 0.7697\n",
      "Validation Loss: -17.1617, Accuracy: 0.8150, F1: 0.8046\n",
      "Best model saved with validation loss: -17.1617\n",
      "\n",
      "\n",
      "Epoch 51, Training Loss: -16.7609\n",
      "Learning Rate: 0.0001895367261420546\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0032\n",
      "shared_transformer_blocks: 0.0101\n",
      "structure_head: 0.0085\n",
      "Class 0 - Precision: 0.9673, Recall: 0.7835, F1-Score: 0.8658\n",
      "Class 1 - Precision: 0.6651, Recall: 0.9132, F1-Score: 0.7697\n",
      "Class 2 - Precision: 0.6581, Recall: 0.9123, F1-Score: 0.7646\n",
      "Validation Loss: -17.0763, Accuracy: 0.8146, F1: 0.8000\n",
      "\n",
      "\n",
      "Epoch 52, Training Loss: -16.8067\n",
      "Learning Rate: 0.0001917004551173413\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0033\n",
      "shared_transformer_blocks: 0.0102\n",
      "structure_head: 0.0084\n",
      "Class 0 - Precision: 0.9630, Recall: 0.8035, F1-Score: 0.8761\n",
      "Class 1 - Precision: 0.6870, Recall: 0.8999, F1-Score: 0.7792\n",
      "Class 2 - Precision: 0.6720, Recall: 0.9086, F1-Score: 0.7726\n",
      "Validation Loss: -16.8052, Accuracy: 0.8174, F1: 0.8093\n",
      "\n",
      "\n",
      "Epoch 53, Training Loss: -16.8378\n",
      "Learning Rate: 0.00019362380282888359\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0033\n",
      "shared_transformer_blocks: 0.0102\n",
      "structure_head: 0.0083\n",
      "Class 0 - Precision: 0.9631, Recall: 0.8024, F1-Score: 0.8754\n",
      "Class 1 - Precision: 0.6868, Recall: 0.8992, F1-Score: 0.7788\n",
      "Class 2 - Precision: 0.6698, Recall: 0.9091, F1-Score: 0.7713\n",
      "Validation Loss: -16.5556, Accuracy: 0.8180, F1: 0.8085\n",
      "\n",
      "\n",
      "Epoch 54, Training Loss: -16.8397\n",
      "Learning Rate: 0.00019530149750525335\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0037\n",
      "shared_transformer_blocks: 0.0103\n",
      "structure_head: 0.0083\n",
      "Class 0 - Precision: 0.9657, Recall: 0.8005, F1-Score: 0.8754\n",
      "Class 1 - Precision: 0.6839, Recall: 0.9092, F1-Score: 0.7806\n",
      "Class 2 - Precision: 0.6718, Recall: 0.9095, F1-Score: 0.7728\n",
      "Validation Loss: -16.9661, Accuracy: 0.8186, F1: 0.8096\n",
      "\n",
      "\n",
      "Epoch 55, Training Loss: -16.8903\n",
      "Learning Rate: 0.00019672894069407143\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0033\n",
      "shared_transformer_blocks: 0.0102\n",
      "structure_head: 0.0082\n",
      "Class 0 - Precision: 0.9690, Recall: 0.7838, F1-Score: 0.8666\n",
      "Class 1 - Precision: 0.6687, Recall: 0.9155, F1-Score: 0.7729\n",
      "Class 2 - Precision: 0.6571, Recall: 0.9165, F1-Score: 0.7654\n",
      "Validation Loss: -17.1505, Accuracy: 0.8176, F1: 0.8016\n",
      "\n",
      "\n",
      "Epoch 56, Training Loss: -16.9361\n",
      "Learning Rate: 0.00019790221986606775\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0035\n",
      "shared_transformer_blocks: 0.0103\n",
      "structure_head: 0.0082\n",
      "Class 0 - Precision: 0.9675, Recall: 0.7916, F1-Score: 0.8708\n",
      "Class 1 - Precision: 0.6769, Recall: 0.9090, F1-Score: 0.7760\n",
      "Class 2 - Precision: 0.6631, Recall: 0.9175, F1-Score: 0.7698\n",
      "Validation Loss: -16.9950, Accuracy: 0.8188, F1: 0.8055\n",
      "\n",
      "\n",
      "Epoch 57, Training Loss: -16.9366\n",
      "Learning Rate: 0.0001988181191390703\n",
      "Average Gradient Norms per Layer:\n",
      "dna_projection: 0.0034\n",
      "shared_transformer_blocks: 0.0102\n",
      "structure_head: 0.0081\n",
      "Class 0 - Precision: 0.9654, Recall: 0.8013, F1-Score: 0.8757\n",
      "Class 1 - Precision: 0.6820, Recall: 0.9061, F1-Score: 0.7782\n",
      "Class 2 - Precision: 0.6746, Recall: 0.9112, F1-Score: 0.7752\n",
      "Validation Loss: -17.3036, Accuracy: 0.8203, F1: 0.8097\n",
      "Best model saved with validation loss: -17.3036\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, train_accuracies = [], [], []\n",
    "val_losses, val_f1s, val_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    layer_grad_norms = {}\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_X, batch_y_struct in train_dataloader:\n",
    "        batch_X, batch_y_struct = batch_X.to(device), batch_y_struct.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred_struct = model(batch_X)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = loss_fn_struct(pred_struct.view(-1, 3), batch_y_struct.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradient norms by layer name\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                layer_name = name.split('.')[0]\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                if layer_name not in layer_grad_norms:\n",
    "                    layer_grad_norms[layer_name] = []\n",
    "                layer_grad_norms[layer_name].append(grad_norm)\n",
    "\n",
    "        # Gradient clipping\n",
    "        nn_utils.clip_grad_value_(model.parameters(), clip_value=0.1)\n",
    "\n",
    "        # Optimizer and scheduler step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Collect true and predicted labels\n",
    "        y_true = batch_y_struct.view(-1)\n",
    "        y_pred = torch.argmax(pred_struct.view(-1, 3), dim=-1)\n",
    "        all_true_labels.append(y_true.cpu())\n",
    "        all_predicted_labels.append(y_pred.cpu())\n",
    "\n",
    "        # Calculate correct predictions\n",
    "        correct_preds += (y_pred == y_true).sum().item()\n",
    "        total_preds += y_true.size(0)\n",
    "\n",
    "    # Calculate training metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    all_true_labels = torch.cat(all_true_labels)\n",
    "    all_predicted_labels = torch.cat(all_predicted_labels)\n",
    "    train_metrics = calculate_class_metrics(all_true_labels, all_predicted_labels)\n",
    "    avg_train_f1 = print_validation_metrics(train_metrics, printer=False)\n",
    "    accuracy = correct_preds / total_preds\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    train_f1s.append(avg_train_f1)\n",
    "\n",
    "    # Calculate average gradient norms per layer\n",
    "    avg_layer_grad_norms = {layer: sum(norms) / len(norms) for layer, norms in layer_grad_norms.items()}\n",
    "\n",
    "    # Print epoch summary with average layer gradient norms\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    print(\"Average Gradient Norms per Layer:\")\n",
    "    for layer, avg_norm in avg_layer_grad_norms.items():\n",
    "        print(f\"{layer}: {avg_norm:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y_struct in val_dataloader:\n",
    "            val_X, val_y_struct = val_X.to(device), val_y_struct.to(device)\n",
    "            val_pred_struct = model(val_X)\n",
    "            val_loss += loss_fn_struct(val_pred_struct.view(-1, 3), val_y_struct.view(-1)).item()\n",
    "\n",
    "            # Collect true and predicted labels\n",
    "            y_true = val_y_struct.view(-1)\n",
    "            y_pred = torch.argmax(val_pred_struct.view(-1, 3), dim=-1)\n",
    "            all_y_true.append(y_true.cpu())\n",
    "            all_y_pred.append(y_pred.cpu())\n",
    "\n",
    "            # Calculate correct predictions\n",
    "            correct_preds += (y_pred == y_true).sum().item()\n",
    "            total_preds += y_true.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    all_y_true = torch.cat(all_y_true)\n",
    "    all_y_pred = torch.cat(all_y_pred)\n",
    "    val_metrics = calculate_class_metrics(all_y_true, all_y_pred)\n",
    "    avg_val_f1 = print_validation_metrics(val_metrics)\n",
    "    val_accuracy = correct_preds / total_preds\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1s.append(avg_val_f1)\n",
    "\n",
    "    # Print validation summary\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {avg_val_f1:.4f}\")\n",
    "\n",
    "    # Check for early stopping and save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"12-2_2D_vecenc.pth\")\n",
    "        print(f\"Best model saved with validation loss: {avg_val_loss:.4f}\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a5367-3079-48d4-96d7-5b3d36f853c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save lists to CSV files with filenames related to the model name\n",
    "model_name = \"12-2_2D_vecenc\"\n",
    "\n",
    "# Create a dictionary for each list to be saved\n",
    "data_to_save = {\n",
    "    f\"{model_name}_train_losses.csv\": train_losses,\n",
    "    f\"{model_name}_train_f1s.csv\": train_f1s,\n",
    "    f\"{model_name}_train_accuracies.csv\": train_accuracies,\n",
    "    f\"{model_name}_val_losses.csv\": val_losses,\n",
    "    f\"{model_name}_val_f1s.csv\": val_f1s,\n",
    "    f\"{model_name}_val_accuracies.csv\": val_accuracies,\n",
    "}\n",
    "\n",
    "# Loop through each list and save it as a CSV\n",
    "for filename, data_list in data_to_save.items():\n",
    "    df = pd.DataFrame(data_list, columns=[filename.split('_')[-1].split('.')[0]])  # Use the metric name as column\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "print(\"Metrics saved to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bc70f-6404-4432-8519-f5b8ae844ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"12-2_2D_vecenc.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Function to decode structure\n",
    "def decode_structure(encoded):\n",
    "    structure_mapping = {0: '.', 1: '(', 2: ')', 3: '-'}  # '-' represents padding\n",
    "    return ''.join([structure_mapping[code.item()] for code in encoded])\n",
    "\n",
    "def enforce_symmetry_and_minimum_distance(pred_structure, min_distance=3):\n",
    "    stack = []\n",
    "    for i, char in enumerate(pred_structure):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                opening_index = stack.pop()\n",
    "                if i - opening_index - 1 < min_distance or ''.join(pred_structure[opening_index + 1:i]).count('.') < min_distance:\n",
    "                    pred_structure[opening_index] = '.'\n",
    "                    pred_structure[i] = '.'\n",
    "    for i in stack:\n",
    "        pred_structure[i] = '.'\n",
    "    return pred_structure\n",
    "\n",
    "\n",
    "num_test_sequences = 50\n",
    "val_sequences = X_val[:num_test_sequences]\n",
    "val_structures = y_struct_val[:num_test_sequences]\n",
    "\n",
    "# Run predictions and compare with ground truth\n",
    "print(\"Testing the model on some sequences from the validation set:\")\n",
    "with torch.no_grad():\n",
    "    for i, (sequence, true_structure) in enumerate(zip(val_sequences, val_structures)):\n",
    "        sequence = sequence.unsqueeze(0).to(device)\n",
    "        true_structure = true_structure.to(device)\n",
    "        \n",
    "        # Predict structure\n",
    "        pred_structure_logits = model(sequence, true_structure.unsqueeze(0))\n",
    "        pred_structure = torch.argmax(pred_structure_logits.view(-1, 3), dim=-1)\n",
    "        \n",
    "        # Decode sequence and structures\n",
    "        decoded_sequence = ''.join([list('ATCG-')[x.item()] for x in sequence[0]])\n",
    "        decoded_true_structure = decode_structure(true_structure)\n",
    "        decoded_pred_structure = decode_structure(pred_structure)\n",
    "\n",
    "        # Enforce symmetry and minimum distance rule\n",
    "        decoded_pred_structure = enforce_symmetry_and_minimum_distance(list(decoded_pred_structure))\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nSequence {i + 1}: {decoded_sequence}\")\n",
    "        print(f\"True Structure:    {decoded_true_structure}\")\n",
    "        print(f\"Predicted Structure: {''.join(decoded_pred_structure)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad42b62-2112-4dbc-babf-7ad0ce1a71bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd878a-7427-4e67-ac28-73ca3c8d160b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
