{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83acf3e0-07d8-4d89-b480-db1c78c056ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of combined dataset after filtering: 1294216\n",
      "Training set size: 1292921\n",
      "Test set size: 1295\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load all datasets\n",
    "with open('database1_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database1_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database2_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database2_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database3_allstructs_34ave.pkl', 'rb') as file:\n",
    "    database3_allstructs_34ave = pickle.load(file)\n",
    "\n",
    "with open('database4_1structsonly_34ave.pkl', 'rb') as file:\n",
    "    database4_1structsonly_34ave = pickle.load(file)\n",
    "\n",
    "with open('database5_1structsonly_50ave.pkl', 'rb') as file:\n",
    "    database5_1structsonly_50ave = pickle.load(file)\n",
    "\n",
    "with open('database6_allstructs_50ave.pkl', 'rb') as file:\n",
    "    database6_allstructs_50ave = pickle.load(file)\n",
    "\n",
    "with open('database7_allstructs_50ave.pkl', 'rb') as file:\n",
    "    database7_allstructs_50ave = pickle.load(file)\n",
    "\n",
    "with open('database9_allstructs_40ave.pkl', 'rb') as file:\n",
    "    database9_allstructs_40ave = pickle.load(file)\n",
    "\n",
    "with open('database10_allstructs_40ave.pkl', 'rb') as file:\n",
    "    database10_allstructs_40ave = pickle.load(file)\n",
    "\n",
    "with open('database11_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database11_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database13_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database13_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database14_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database14_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database15_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database15_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database16_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database16_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database17_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database17_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database18_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database18_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database19_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database19_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database20_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database20_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database21_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database21_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database22_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database22_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database23_allstructs_45ave.pkl', 'rb') as file:\n",
    "    database23_allstructs_45ave = pickle.load(file)\n",
    "\n",
    "with open('database24_allstructs_45only.pkl', 'rb') as file:\n",
    "    database24_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database25_allstructs_45only.pkl', 'rb') as file:\n",
    "    database25_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database26_allstructs_45only.pkl', 'rb') as file:\n",
    "    database26_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database27_allstructs_45only.pkl', 'rb') as file:\n",
    "    database27_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database28_allstructs_45only.pkl', 'rb') as file:\n",
    "    database28_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database29_allstructs_45only.pkl', 'rb') as file:\n",
    "    database29_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database30_allstructs_45only.pkl', 'rb') as file:\n",
    "    database30_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database31_allstructs_45only.pkl', 'rb') as file:\n",
    "    database31_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database32_allstructs_45only.pkl', 'rb') as file:\n",
    "    database32_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database33_allstructs_45only.pkl', 'rb') as file:\n",
    "    database33_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database34_allstructs_45only.pkl', 'rb') as file:\n",
    "    database34_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database35_allstructs_45only.pkl', 'rb') as file:\n",
    "    database35_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database36_allstructs_45only.pkl', 'rb') as file:\n",
    "    database36_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database37_allstructs_45only.pkl', 'rb') as file:\n",
    "    database37_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database38_allstructs_45only.pkl', 'rb') as file:\n",
    "    database38_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database39_allstructs_45only.pkl', 'rb') as file:\n",
    "    database39_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database40_allstructs_45only.pkl', 'rb') as file:\n",
    "    database40_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database41_allstructs_45only.pkl', 'rb') as file:\n",
    "    database41_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database42_allstructs_45only.pkl', 'rb') as file:\n",
    "    database42_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database43_allstructs_45only.pkl', 'rb') as file:\n",
    "    database43_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database44_allstructs_45only.pkl', 'rb') as file:\n",
    "    database44_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database45_allstructs_45only.pkl', 'rb') as file:\n",
    "    database45_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database46_allstructs_45only.pkl', 'rb') as file:\n",
    "    database46_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database47_allstructs_45only.pkl', 'rb') as file:\n",
    "    database47_allstructs_45only = pickle.load(file)\n",
    "\n",
    "with open('database48_allstructs_45only.pkl', 'rb') as file:\n",
    "    database48_allstructs_45only = pickle.load(file)\n",
    "    \n",
    "\n",
    "# Combine datasets into a single dictionary\n",
    "combined_data = {}\n",
    "datasets = [\n",
    "    database1_allstructs_34ave, database2_allstructs_34ave, database3_allstructs_34ave,\n",
    "    database4_1structsonly_34ave, database5_1structsonly_50ave, database6_allstructs_50ave,\n",
    "    database7_allstructs_50ave, database9_allstructs_40ave, database10_allstructs_40ave, \n",
    "    database11_allstructs_45ave, database13_allstructs_45ave, database14_allstructs_45ave,\n",
    "    database15_allstructs_45ave, database16_allstructs_45ave, database17_allstructs_45ave,\n",
    "    database18_allstructs_45ave, database19_allstructs_45ave, database20_allstructs_45ave,\n",
    "    database21_allstructs_45ave, database22_allstructs_45ave, database23_allstructs_45ave,\n",
    "    database24_allstructs_45only, database25_allstructs_45only, database26_allstructs_45only,\n",
    "    database27_allstructs_45only, database28_allstructs_45only, database29_allstructs_45only,\n",
    "    database30_allstructs_45only, database31_allstructs_45only, database32_allstructs_45only,\n",
    "    database33_allstructs_45only, database34_allstructs_45only, database35_allstructs_45only,\n",
    "    database36_allstructs_45only, database37_allstructs_45only, database38_allstructs_45only,\n",
    "    database39_allstructs_45only, database40_allstructs_45only, database41_allstructs_45only,\n",
    "    database42_allstructs_45only, database43_allstructs_45only, database44_allstructs_45only,\n",
    "    database45_allstructs_45only, database46_allstructs_45only, database47_allstructs_45only,\n",
    "    database48_allstructs_45only\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seq, (temp, structs) in dataset.items():\n",
    "        # Only include if melting temperature is defined and above 20\n",
    "        if temp is not None:\n",
    "            combined_data[seq] = (temp, structs)\n",
    "\n",
    "combined_data2 = {}\n",
    "for i, j in combined_data.items():\n",
    "    if len(i) == len(j[1][-1]):\n",
    "        combined_data2[i] = j\n",
    "\n",
    "combined_data = combined_data2\n",
    "\n",
    "# Verify combined data size after filtering\n",
    "print(f\"Total size of combined dataset after filtering: {len(combined_data)}\")\n",
    "\n",
    "# Prepare data for train-test split\n",
    "sequences = list(combined_data.keys())\n",
    "labels = list(combined_data.values())\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% test)\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Create train and test sets as dictionaries\n",
    "train_set = {seq: label for seq, label in zip(train_sequences, train_labels)}\n",
    "test_set = {seq: label for seq, label in zip(test_sequences, test_labels)}\n",
    "\n",
    "# Display the split summary\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994de9d2-83ea-4ca5-ba13-cdd20810a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, j in train_set.items():\n",
    "    if len(i) != len(j[1][-1]):\n",
    "        count +=1 \n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2423e75a-bf43-46fd-b32c-2d0084162646",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_sequences = [seq for seq in train_set.keys() if len(seq) == 45]\n",
    "top_structures = [train_set[seq][1] for seq in dna_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f745fd09-e181-473b-b036-bbd728f49f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777256\n"
     ]
    }
   ],
   "source": [
    "print(len(dna_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef21692e-1c22-4c59-9658-1ba8341a9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGGGGGATACATGTTTCTTATATAGAGTACGTGACGTGCCCTTTT',\n",
       " 'TCCTGGCGGTTCAAGGATACGATGCCTGCCTTCATTTTTGAGCGA',\n",
       " 'AAGCACTGCCTTCTCCTTATAACAACTATGTGTTATGTCTGTTTA',\n",
       " 'TATCTCAAACCACGGCTTTTCAACAAAATTGTGACGACATGCGCC',\n",
       " 'AGCGTAGTGGAGGTAGAAGTCTTTAAGGCGATCATCTTATCATCA',\n",
       " 'GTTAAGTATTGTCTCAGTAGACATGGTAGTACGCGTGGGATGAGG',\n",
       " 'CTGGCATTCTGTTGCCGAGCCCATTGGTGCATCGTCGTGACAGGG',\n",
       " 'GGTAGACGATACAGAGCTCGAATTCTGCTATGCTGGGACGCTCTG',\n",
       " 'GTAGCGAGAAGCTAAATGGCCCGAAACGTCTAGGAGTGTCCGGGC',\n",
       " 'CGGTGCGTCGGAGGATTGGGATCACTATCCATACGGGGGCTAATT',\n",
       " 'AAAAACATGACGACTAGCCTGTACATTTTAAAGATATTGACGCCT',\n",
       " 'GACCGCTTGGTAATAGTGGGTTTTGTGAGAAAAGAACCACCATGG',\n",
       " 'GGGGTACCTAAAAGGATTGGGCTTTTCCTCTTAGAGCTCTACACA',\n",
       " 'GTAGAGCGGGATAGCTTTCAATCTGTATATGTGCCAATTAGCTGA',\n",
       " 'GGGGTTGGCAAAGCCGAAGGACCAACTATACATTAGTGAACTTAA',\n",
       " 'ACGACTCCTGGGCCAGCGTAATAACGCTTCTGTACCACCCTATCG',\n",
       " 'CATTGGCACCGTACTGGTCTGAAACACTCCCCTCTAATTAGTTAC',\n",
       " 'AGTTTATGCGGGCGTAACCTGATGTCTCCCATACCCAATTACTTT',\n",
       " 'GGAGTTGAACGGAGGGGGCCCCCTATCACGAGCCCGCTCTACGGC',\n",
       " 'AACCTTGGGACGACCTGTCTGGTCTAGAAGAAGTGCATACCCAAT',\n",
       " 'TCGGAAGAACAGGACTACCTACTCAAGCGTTAAGAAGTACATATC',\n",
       " 'GCTTTACCGGGTCTGCACTATAGATACATTTGTCGGAGATTAAAA',\n",
       " 'AAAATAATGTTACTAACGCGCACTCGATCACGCCTCTATTAGCCT',\n",
       " 'TTCACGTCAAACGGCGTTTCCGCTACGTGGCGTTACTTTAAACCT',\n",
       " 'AGGATGGCCGATCCACTGGTGATTCAGTTCACAACTGTGGGTTTG',\n",
       " 'GTTCGATCATAATTGTCAGAGCATAGTATCTCTTGTGCTACCCAA',\n",
       " 'TAGGACAATCCGGGTTGTGACTCGGTAACGTCAATGACTCCTAAC',\n",
       " 'TTTGAGTATAACTAAACATGTCTCCCGCTCGCTATTATGCACCCC',\n",
       " 'GTGCGATAGGTCGATCACGGAGTGGTCCGCGTCATTATAGGTACG',\n",
       " 'TGTCATCGCGGCATAGGCTATTACAAACATTAACTATGCGTCGTT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_sequences[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c93c17-7328-4b0f-909a-8eed3ffee0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['((((.....(((((.(((.....)))..))))).....))))...',\n",
       "  '..(((.((.(((((.(((.....)))..)))))..)).)))....'],\n",
       " ['.((....))....(((........))).....((....)).....',\n",
       "  '.(((.........)))................((....)).....',\n",
       "  '....((((..............)))).((..(((....)))))..',\n",
       "  '....(((......(((........))))))..((....)).....',\n",
       "  '....(((((.((........))..)).)))..((....)).....',\n",
       "  '....((((..............))))......((....)).....',\n",
       "  '....((((((.............))).)))..((....)).....',\n",
       "  '....(((((..((.........)).)))))..((....)).....'],\n",
       " ['.((............)).((((((......)))))).........',\n",
       "  '.((.........))....((((((......)))))).........',\n",
       "  '(((......)))......((((((......)))))).........',\n",
       "  '..((...)).........((((((......)))))).........'],\n",
       " ['......(((........)))..(((....)))..((.....))..',\n",
       "  '....................(((.....)))...((.....))..',\n",
       "  '..........((((..((((....)))).)))).((.....))..',\n",
       "  '......................(((....)))..((.....))..'],\n",
       " ['................((....)).....(((......)))....',\n",
       "  '...............((.(((.....)))..))............',\n",
       "  '..............(((.................)))........',\n",
       "  '..(.....).....((....)).......(((......)))....',\n",
       "  '................(((................))).......',\n",
       "  '..............((....)).......(((.........))).',\n",
       "  '..(((......................)))...............',\n",
       "  '..(...).......((....)).......(((......)))....',\n",
       "  '..............((....))..(((........))).......',\n",
       "  '..............((....)).......(((......)))....'],\n",
       " ['...((...))((((....)))).......................',\n",
       "  '..........((((....))))...((...)).............',\n",
       "  '..........((((....)))).........(.........)...',\n",
       "  '.....(((((((((....))))).....)))).............',\n",
       "  '..........((((....))))........(....).........',\n",
       "  '..........((((....))))...........((.....))...',\n",
       "  '..........((((....)))).......................'],\n",
       " ['..((((......)))).........(...)...((....))....',\n",
       "  '..((((......))))..((........)).....(......)..',\n",
       "  '((((((......)))).))..............((....))....',\n",
       "  '........((((((((((((........)).)))..)))))))..',\n",
       "  '..((((......))))...(((.(...............)..)))',\n",
       "  '..((((......)))).....((..............))......',\n",
       "  '........((((...((((((....)))......)))..))))..',\n",
       "  '..((((......))))...(((.....((...........)))))',\n",
       "  '..(((.............)))............((....))....',\n",
       "  '..((((......))))..((........))..(((....)).)..',\n",
       "  '..((((......))))...(((..((.(((......))).)))))',\n",
       "  '..((((......))))...(((..((..............)))))',\n",
       "  '..((((......))))...((....))....((.....)).....',\n",
       "  '..((((......)))).....((....))....((....))....',\n",
       "  '..((((......))))...((....))..((......))......',\n",
       "  '..((((......))))...(((....................)))',\n",
       "  '..((((......))))...((....))......((....))....',\n",
       "  '..((((......))))..((........))...((....))....'],\n",
       " ['.(((.....)))..(((..............)))...........',\n",
       "  '......(((........)))...........((.(...)))....',\n",
       "  '...........((.(((.........))).)).............',\n",
       "  '...........((((........))))....((.(...)))....',\n",
       "  '...........((((((..(...((.((...)).))..)))))))',\n",
       "  '.(((.....)))..(((.........)))..((.(...)))....',\n",
       "  '...........((((((((.................)).))))))',\n",
       "  '...........((((((.......(.((...)).)....))))))',\n",
       "  '...........((((((......................))))))'],\n",
       " ['..(((.....))).....((((((....)...........)))))',\n",
       "  '......((...)).....(((((..((..........)).)))))',\n",
       "  '(((((.....))))..).(((((..((..........)).)))))',\n",
       "  '..(((.....))).....(((((..((....(....))).)))))',\n",
       "  '..(((.....))).....(((((..........(.....))))))',\n",
       "  '..(((.....))).....(((((..((..(.....).)).)))))',\n",
       "  '..(((.....))).....(((((..((..........)).)))))'],\n",
       " ['......(((....)))..((((....))))...............',\n",
       "  '....((.(((..((((..(.....).))))...)))..)).....',\n",
       "  '.............(((....))).....((.....))........',\n",
       "  '....((.(((..((((((....))..))))...)))..)).....',\n",
       "  '((...))...........((((....))))...............',\n",
       "  '....((.(((..((((..........))))...)))..)).....',\n",
       "  '.....(((.(((.(((....)))....)))..)))..........',\n",
       "  '.....(((.(((...............)))..)))..........',\n",
       "  '........(...).....((((....))))...............',\n",
       "  '((......))........((((....))))...............',\n",
       "  '..................((((....))))...............']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_structures[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e9ad8-ec71-4953-a369-33e195ae0407",
   "metadata": {},
   "source": [
    "# Some Parameters Changed and Only Sequences 40-50 bp long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21626030-fe7c-4a16-9af9-c4686ec0e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Number of sequences in dna_sequences:  777256\n",
      "Sample weights stats:\n",
      "Min: 0.1343396008014679, Max: 0.3759748339653015, Mean: 0.22687210142612457\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "# Parameters\n",
    "sequence_length = 45\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_shared_transformer_blocks = 12\n",
    "num_task_transformer_blocks = 6\n",
    "dropout_rate = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Encoding functions with Modified One-hot Encoding with Pairing Bias\n",
    "def encode_dna_sequence(seq):\n",
    "    # Modified one-hot encoding with pairing bias for A-T and C-G\n",
    "    encoding_dict = {\n",
    "        'A': [0.8, 0.2, 0.1, 0.1],  # A-T bias\n",
    "        'T': [0.2, 0.8, 0.1, 0.1],  # T-A bias\n",
    "        'C': [0.1, 0.1, 0.8, 0.2],  # C-G bias\n",
    "        'G': [0.1, 0.1, 0.2, 0.8]   # G-C bias\n",
    "    }\n",
    "    return [encoding_dict[base] for base in seq]\n",
    "\n",
    "def encode_structure(structure):\n",
    "    structure_mapping = {'.': 0, '(': 1, ')': 2}\n",
    "    return [structure_mapping[char] for char in structure]\n",
    "\n",
    "# Assuming train_set is available in the format where dna sequences are keys\n",
    "dna_sequences = [seq for seq in train_set.keys() if len(seq) == 45]\n",
    "print(\"Number of sequences in dna_sequences: \", len(dna_sequences))\n",
    "top_structures = [train_set[seq][1] for seq in dna_sequences]\n",
    "\n",
    "#dna_sequences = dna_sequences[:100]\n",
    "#top_structures = top_structures[:100]\n",
    "\n",
    "# Encode DNA sequences with modified one-hot encoding (no padding)\n",
    "encoded_sequences = []\n",
    "for seq in dna_sequences:\n",
    "    encoded_seq = encode_dna_sequence(seq)  # Directly encode the sequence without padding\n",
    "    encoded_sequences.append(encoded_seq)\n",
    "\n",
    "# Encode structures with 3\n",
    "encoded_structures = []\n",
    "for structs in top_structures:\n",
    "    structure = structs[-1]\n",
    "    encoded = encode_structure(structure)\n",
    "    encoded_structures.append(encoded)\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(encoded_sequences, dtype=torch.float32)  # Use float32 since one-hot encoding uses floats\n",
    "y_struct = torch.tensor(encoded_structures, dtype=torch.long)\n",
    "\n",
    "X_train, X_val, y_struct_train, y_struct_val = train_test_split(X, y_struct, test_size=0.2, random_state=42)\n",
    "\n",
    "\"\"\"\n",
    "# Calculate class counts based on encoded structures\n",
    "class_counts = torch.tensor(\n",
    "    [sum(struct.count(char) for struct in [\"\".join(top_struct) for top_struct in top_structures]) for char in \".()\"], \n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Ensure no division by zero\n",
    "total_count = class_counts.sum()\n",
    "if total_count == 0 or any(class_counts == 0):\n",
    "    raise ValueError(\"Class counts are invalid, resulting in division by zero.\")\n",
    "\n",
    "# Compute class weights (Inverse frequency weighting)\n",
    "class_weights = total_count / (class_counts + 1e-5)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.clamp(class_weights, min=0)  # Ensure non-negative weights\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Create weighted sampler for the training set\n",
    "sample_weights = []\n",
    "for y in y_struct_train:\n",
    "    valid_indices = y  # Here we are using y directly as it represents the class labels for each base\n",
    "    weight = [class_weights[class_idx].item() for class_idx in valid_indices]\n",
    "    sample_weights.append(sum(weight) / len(weight))  # Average weight for the sequence\n",
    "\n",
    "# Convert to tensor and validate\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "sample_weights = torch.nan_to_num(sample_weights, nan=1.0, posinf=1.0, neginf=1.0)  # Replace NaN/infs with default value\n",
    "sample_weights = torch.clamp(sample_weights, min=0)  # Ensure weights are non-negative\"\"\"\n",
    "\n",
    "class_weights = torch.load(\"class_weights_only45s.pt\").to(device)\n",
    "sample_weights = torch.load(\"sample_weights_only45s.pt\").to(device)\n",
    "\n",
    "\n",
    "# Debugging: Print sample weights stats\n",
    "print(\"Sample weights stats:\")\n",
    "print(f\"Min: {sample_weights.min()}, Max: {sample_weights.max()}, Mean: {sample_weights.mean()}\")\n",
    "assert torch.all(sample_weights >= 0), \"Sample weights contain negative values!\"\n",
    "assert not torch.any(torch.isnan(sample_weights)), \"Sample weights contain NaN values!\"\n",
    "\n",
    "# Create sampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_struct_train)\n",
    "val_dataset = TensorDataset(X_val, y_struct_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4f57ca6-0ce2-40d4-80ed-04fb55759afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define the Transformer Encoder Block with Pairing Attention\n",
    "class TransformerEncoderBlockWithPairingAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.2, sequence_length=45):\n",
    "        super(TransformerEncoderBlockWithPairingAttention, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.pairing_bias = nn.Parameter(torch.randn(sequence_length, sequence_length))\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the multi-head attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        \n",
    "        # Compute the attention scores with the self-attention mechanism\n",
    "        q, k, v = x, x, x\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        \n",
    "        # Apply pairing bias to the attention scores\n",
    "        seq_len = q.size(1)\n",
    "        pairing_bias_resized = self.pairing_bias[:seq_len, :seq_len]\n",
    "        pairing_bias_resized = pairing_bias_resized.unsqueeze(0).expand(x.size(0), -1, -1).to(x.device)\n",
    "        attn_scores = attn_scores + pairing_bias_resized\n",
    "        \n",
    "        # Normalize the attention scores and compute the weighted sum\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        paired_attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Apply dropout and residual connection for the attention output\n",
    "        x = x + self.dropout1(attn_output + paired_attn_output)\n",
    "        x = self.layernorm1(x)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        x = x + self.dropout2(self.ffn(x))\n",
    "        x = self.layernorm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StructurePredictor(nn.Module):\n",
    "    def __init__(self, sequence_length, embedding_dim, num_heads, ff_dim, num_shared_blocks, num_task_blocks):\n",
    "        super(StructurePredictor, self).__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Projection layer to increase the embedding dimension from 4 to 256\n",
    "        self.projection = nn.Linear(4, embedding_dim)\n",
    "\n",
    "        # Initialize shared transformer blocks with embedding_dim = 256\n",
    "        self.shared_transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlockWithPairingAttention(embedding_dim, num_heads, ff_dim, sequence_length=sequence_length)\n",
    "            for _ in range(num_shared_blocks)\n",
    "        ])\n",
    "\n",
    "        # Initialize task-specific transformer blocks\n",
    "        self.struct_transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlockWithPairingAttention(embedding_dim, num_heads, ff_dim, sequence_length=sequence_length)\n",
    "            for _ in range(num_task_transformer_blocks)\n",
    "        ])\n",
    "\n",
    "        # Structure head for final predictions (paired \"(\", unpaired \".\", and closing \")\")\n",
    "        self.structure_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 3)  # Output 3 classes: unpaired (.), paired ((), and closing ()) \n",
    "        )\n",
    "\n",
    "    def create_sinusoidal_positional_encoding(self, seq_len, d_model):\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pos_enc = torch.zeros(seq_len, d_model)\n",
    "        pos_enc[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(pos * div_term)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, encoded_structure=None):\n",
    "        device = x.device  # Get the device of input tensor\n",
    "\n",
    "        # Project the input from 4 to 256 dimensions\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Ensure positional encodings are the correct shape to be added to x\n",
    "        positional_encoding = self.create_sinusoidal_positional_encoding(x.size(1), self.embedding_dim).to(device)\n",
    "        sinusoidal_positional_encoding = positional_encoding  # Reusing the same positional encoding\n",
    "\n",
    "        # Add positional encodings to input embeddings (x)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0).expand(x.size(0), -1, -1)  # Expand to match batch size\n",
    "        x = x + positional_encoding  # Add positional encoding\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # Permute for transformer compatibility\n",
    "\n",
    "        # Apply shared transformer blocks\n",
    "        for block in self.shared_transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Output layer (structure prediction)\n",
    "        output = self.structure_head(x.permute(1, 0, 2))  # Permute back to original shape\n",
    "        return output\n",
    "\n",
    "\n",
    "class RewardedThermodynamicallyBalancedCategoricalCrossEntropy(nn.Module):\n",
    "    def __init__(self, weights, pairing_penalty=0.2, thermo_penalty=0.3, specificity_reward=0.05):\n",
    "        super(RewardedThermodynamicallyBalancedCategoricalCrossEntropy, self).__init__()\n",
    "        self.weights = weights  # Weights for the categorical cross-entropy loss\n",
    "        self.pairing_penalty = pairing_penalty  # Penalty for pairing imbalance\n",
    "        self.thermo_penalty = thermo_penalty  # Penalty for thermodynamic mismatch\n",
    "        self.specificity_reward = specificity_reward  # Reward for specificity (correct pairings)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Convert y_true to a 3-class one-hot representation (\".\", \"(\", \")\")\n",
    "        y_true_one_hot = F.one_hot(y_true, num_classes=3).float()  # One-hot encoding for 3 classes\n",
    "\n",
    "        # Calculate log-probabilities for predictions\n",
    "        log_probs = F.log_softmax(y_pred, dim=-1)\n",
    "\n",
    "        # Cross-entropy loss calculation\n",
    "        loss = -torch.sum(self.weights * y_true_one_hot * log_probs, dim=-1).mean()\n",
    "\n",
    "        # Pairing imbalance penalty: penalize imbalance between paired and unpaired bases\n",
    "        pred_labels = torch.argmax(y_pred, dim=-1)  # Get the predicted class labels (0, 1, or 2)\n",
    "        open_count = (pred_labels == 1).sum()  # Count of open brackets \"(\" (1)\n",
    "        close_count = (pred_labels == 2).sum()  # Count of closed brackets \")\" (2)\n",
    "        \n",
    "        # Calculate the pairing imbalance penalty\n",
    "        imbalance_penalty = self.pairing_penalty * torch.abs(open_count - close_count).float()\n",
    "\n",
    "        # Thermodynamic penalty: penalize mispairing (open vs close mismatches)\n",
    "        mismatch_penalty = torch.sum((pred_labels == 1) & (y_true == 2)) * self.thermo_penalty\n",
    "\n",
    "        # Specificity reward: reward correct hairpin/stem predictions\n",
    "        correct_pairings = ((pred_labels == y_true) & ((y_true == 1) | (y_true == 2))).sum()\n",
    "        specificity_reward = self.specificity_reward * correct_pairings.float()\n",
    "\n",
    "        # Final loss: includes the cross-entropy loss, imbalance penalty, thermodynamic penalty, and specificity reward\n",
    "        return loss + imbalance_penalty + mismatch_penalty - specificity_reward\n",
    "\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = StructurePredictor(sequence_length, embedding_dim, num_heads, ff_dim, num_shared_transformer_blocks, num_task_transformer_blocks).to(device)\n",
    "loss_fn_struct = RewardedThermodynamicallyBalancedCategoricalCrossEntropy(class_weights)  # 4 for padding\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\n",
    "\n",
    "# Training loop with early stopping and metrics\n",
    "best_avg_val_loss = float(\"inf\")\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "def calculate_class_metrics(y_true, y_pred):\n",
    "    # Convert y_pred to the same shape as y_true by selecting the class with the highest logit/probability\n",
    "    y_pred_labels = y_pred  # Shape: [batch_size, sequence_length]\n",
    "    # Ensure y_true has shape [batch_size, sequence_length] as expected (labels should be in range [0, 1, 2])\n",
    "    y_true_labels = y_true  # Since y_true is already in class label format (0, 1, or 2)\n",
    "\n",
    "    # Flatten the tensors to calculate the metrics across all sequences\n",
    "    y_true_flat = y_true_labels.view(-1)  # Flatten to [batch_size * sequence_length]\n",
    "    y_pred_flat = y_pred_labels.view(-1)  # Flatten to [batch_size * sequence_length]\n",
    "\n",
    "    # Check if both tensors have the same size\n",
    "    assert y_true_flat.size(0) == y_pred_flat.size(0), \\\n",
    "        f\"Size mismatch: {y_true_flat.size(0)} vs {y_pred_flat.size(0)}\"\n",
    "\n",
    "    # Initialize dictionaries to store metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # Loop over all classes (0, 1, 2 for \".\", \"(\", and \")\")\n",
    "    for class_idx, class_name in enumerate([0, 1, 2]):  # 0 for \".\", 1 for \"(\", 2 for \")\"\n",
    "        tp = ((y_pred_flat == class_idx) & (y_true_flat == class_idx)).sum().item()  # True positive\n",
    "        fp = ((y_pred_flat == class_idx) & (y_true_flat != class_idx)).sum().item()  # False positive\n",
    "        fn = ((y_pred_flat != class_idx) & (y_true_flat == class_idx)).sum().item()  # False negative\n",
    "        \n",
    "        # Precision and Recall with protection against division by zero\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        \n",
    "        # F1 score calculation\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        # Store metrics for each class\n",
    "        metrics[class_name] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_validation_metrics(val_metrics, printer=True):\n",
    "    \n",
    "    # Initialize a variable to store the sum of F1 scores\n",
    "    total_f1 = 0\n",
    "    num_classes = len(val_metrics)\n",
    "    \n",
    "    # Print metrics for each class\n",
    "    for cls, metrics in val_metrics.items():\n",
    "        if printer==True:\n",
    "            print(f\"Class {cls} - Precision: {metrics['precision']:.4f}, \"\n",
    "                  f\"Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
    "        total_f1 += metrics['f1']\n",
    "    \n",
    "    # Calculate the average F1 score\n",
    "    avg_f1 = total_f1 / num_classes if num_classes > 0 else 0.0\n",
    "    \n",
    "    return avg_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2cad421-5c8e-4439-b0c1-a8ac50b5800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save sample weights\n",
    "torch.save(sample_weights, \"sample_weights_only45s.pt\")\n",
    "\n",
    "# Load sample weights\n",
    "torch.save(class_weights, \"class_weights_only45s.pt\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93449390-4e09-4dd3-a6e1-86a48ab00791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run this cell if the gradients explode!\n",
    "\"\"\"\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.00020, steps_per_epoch=len(train_dataloader), epochs=200)\n",
    "model_path = \"12-2_pairaware_enc.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8eb2ca60-346a-4489-92cf-57934063aa03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Precision: 0.8813, Recall: 0.0852, F1-Score: 0.1553\n",
      "Class 1 - Precision: 0.2513, Recall: 0.6758, F1-Score: 0.3663\n",
      "Class 2 - Precision: 0.2464, Recall: 0.7196, F1-Score: 0.3671\n",
      "Epoch 1/200 - Train Loss: 13.9447, Train Accuracy: 0.3054, Train F1: 0.3114, Val Loss: 13.8654, Val Accuracy: 0.2895, Val F1: 0.2963\n",
      "Model saved with Val F1: 0.2963\n",
      "Class 0 - Precision: 0.7707, Recall: 0.4157, F1-Score: 0.5401\n",
      "Class 1 - Precision: 0.2903, Recall: 0.5689, F1-Score: 0.3844\n",
      "Class 2 - Precision: 0.2965, Recall: 0.5578, F1-Score: 0.3872\n",
      "Epoch 2/200 - Train Loss: 12.5965, Train Accuracy: 0.3131, Train F1: 0.3196, Val Loss: 6.7540, Val Accuracy: 0.4650, Val F1: 0.4372\n",
      "Model saved with Val F1: 0.4372\n",
      "Class 0 - Precision: 0.7948, Recall: 0.3811, F1-Score: 0.5152\n",
      "Class 1 - Precision: 0.2997, Recall: 0.5884, F1-Score: 0.3972\n",
      "Class 2 - Precision: 0.2944, Recall: 0.6232, F1-Score: 0.3999\n",
      "Epoch 3/200 - Train Loss: 7.0793, Train Accuracy: 0.4604, Train F1: 0.4424, Val Loss: 6.8547, Val Accuracy: 0.4561, Val F1: 0.4374\n",
      "Model saved with Val F1: 0.4374\n",
      "Class 0 - Precision: 0.8264, Recall: 0.3170, F1-Score: 0.4582\n",
      "Class 1 - Precision: 0.2904, Recall: 0.6722, F1-Score: 0.4055\n",
      "Class 2 - Precision: 0.2962, Recall: 0.6362, F1-Score: 0.4042\n",
      "Epoch 4/200 - Train Loss: 7.2280, Train Accuracy: 0.4517, Train F1: 0.4398, Val Loss: 10.2525, Val Accuracy: 0.4295, Val F1: 0.4227\n",
      "Class 0 - Precision: 0.8384, Recall: 0.3218, F1-Score: 0.4651\n",
      "Class 1 - Precision: 0.2966, Recall: 0.6470, F1-Score: 0.4068\n",
      "Class 2 - Precision: 0.2948, Recall: 0.6725, F1-Score: 0.4099\n",
      "Epoch 5/200 - Train Loss: 6.6411, Train Accuracy: 0.4473, Train F1: 0.4385, Val Loss: 5.7123, Val Accuracy: 0.4345, Val F1: 0.4273\n",
      "Class 0 - Precision: 0.8360, Recall: 0.3168, F1-Score: 0.4595\n",
      "Class 1 - Precision: 0.2989, Recall: 0.6592, F1-Score: 0.4113\n",
      "Class 2 - Precision: 0.2932, Recall: 0.6676, F1-Score: 0.4075\n",
      "Epoch 6/200 - Train Loss: 6.2676, Train Accuracy: 0.4483, Train F1: 0.4400, Val Loss: 5.8320, Val Accuracy: 0.4324, Val F1: 0.4261\n",
      "Class 0 - Precision: 0.8301, Recall: 0.3543, F1-Score: 0.4967\n",
      "Class 1 - Precision: 0.3002, Recall: 0.6439, F1-Score: 0.4095\n",
      "Class 2 - Precision: 0.3029, Recall: 0.6499, F1-Score: 0.4132\n",
      "Epoch 7/200 - Train Loss: 6.0656, Train Accuracy: 0.4487, Train F1: 0.4406, Val Loss: 5.5823, Val Accuracy: 0.4519, Val F1: 0.4398\n",
      "Model saved with Val F1: 0.4398\n",
      "Class 0 - Precision: 0.8417, Recall: 0.3152, F1-Score: 0.4586\n",
      "Class 1 - Precision: 0.2971, Recall: 0.6720, F1-Score: 0.4121\n",
      "Class 2 - Precision: 0.2973, Recall: 0.6654, F1-Score: 0.4110\n",
      "Epoch 8/200 - Train Loss: 5.8302, Train Accuracy: 0.4495, Train F1: 0.4414, Val Loss: 5.7305, Val Accuracy: 0.4331, Val F1: 0.4272\n",
      "Class 0 - Precision: 0.8368, Recall: 0.3284, F1-Score: 0.4717\n",
      "Class 1 - Precision: 0.2980, Recall: 0.6691, F1-Score: 0.4124\n",
      "Class 2 - Precision: 0.3019, Recall: 0.6589, F1-Score: 0.4141\n",
      "Epoch 9/200 - Train Loss: 5.6297, Train Accuracy: 0.4515, Train F1: 0.4432, Val Loss: 5.6748, Val Accuracy: 0.4404, Val F1: 0.4327\n",
      "Class 0 - Precision: 0.8311, Recall: 0.3541, F1-Score: 0.4966\n",
      "Class 1 - Precision: 0.3029, Recall: 0.6575, F1-Score: 0.4148\n",
      "Class 2 - Precision: 0.3053, Recall: 0.6482, F1-Score: 0.4151\n",
      "Epoch 10/200 - Train Loss: 5.2988, Train Accuracy: 0.4538, Train F1: 0.4450, Val Loss: 5.1662, Val Accuracy: 0.4538, Val F1: 0.4422\n",
      "Model saved with Val F1: 0.4422\n",
      "Class 0 - Precision: 0.8400, Recall: 0.3259, F1-Score: 0.4696\n",
      "Class 1 - Precision: 0.3033, Recall: 0.6581, F1-Score: 0.4152\n",
      "Class 2 - Precision: 0.2975, Recall: 0.6773, F1-Score: 0.4134\n",
      "Epoch 11/200 - Train Loss: 5.2103, Train Accuracy: 0.4542, Train F1: 0.4454, Val Loss: 5.2609, Val Accuracy: 0.4399, Val F1: 0.4328\n",
      "Class 0 - Precision: 0.8305, Recall: 0.3601, F1-Score: 0.5024\n",
      "Class 1 - Precision: 0.3092, Recall: 0.6356, F1-Score: 0.4160\n",
      "Class 2 - Precision: 0.3021, Recall: 0.6671, F1-Score: 0.4159\n",
      "Epoch 12/200 - Train Loss: 5.0246, Train Accuracy: 0.4555, Train F1: 0.4464, Val Loss: 6.0241, Val Accuracy: 0.4572, Val F1: 0.4448\n",
      "Model saved with Val F1: 0.4448\n",
      "Class 0 - Precision: 0.8320, Recall: 0.3535, F1-Score: 0.4962\n",
      "Class 1 - Precision: 0.3074, Recall: 0.6567, F1-Score: 0.4188\n",
      "Class 2 - Precision: 0.3035, Recall: 0.6561, F1-Score: 0.4150\n",
      "Epoch 13/200 - Train Loss: 4.7653, Train Accuracy: 0.4568, Train F1: 0.4476, Val Loss: 4.2781, Val Accuracy: 0.4545, Val F1: 0.4433\n",
      "Class 0 - Precision: 0.8405, Recall: 0.3203, F1-Score: 0.4639\n",
      "Class 1 - Precision: 0.3022, Recall: 0.6605, F1-Score: 0.4146\n",
      "Class 2 - Precision: 0.2987, Recall: 0.6832, F1-Score: 0.4157\n",
      "Epoch 14/200 - Train Loss: 4.5946, Train Accuracy: 0.4580, Train F1: 0.4485, Val Loss: 4.4425, Val Accuracy: 0.4376, Val F1: 0.4314\n",
      "Class 0 - Precision: 0.8292, Recall: 0.3688, F1-Score: 0.5105\n",
      "Class 1 - Precision: 0.3102, Recall: 0.6478, F1-Score: 0.4195\n",
      "Class 2 - Precision: 0.3070, Recall: 0.6541, F1-Score: 0.4179\n",
      "Epoch 15/200 - Train Loss: 4.5268, Train Accuracy: 0.4586, Train F1: 0.4491, Val Loss: 3.8704, Val Accuracy: 0.4629, Val F1: 0.4493\n",
      "Model saved with Val F1: 0.4493\n",
      "Class 0 - Precision: 0.8298, Recall: 0.3651, F1-Score: 0.5071\n",
      "Class 1 - Precision: 0.3074, Recall: 0.6562, F1-Score: 0.4187\n",
      "Class 2 - Precision: 0.3084, Recall: 0.6487, F1-Score: 0.4181\n",
      "Epoch 16/200 - Train Loss: 4.4933, Train Accuracy: 0.4596, Train F1: 0.4499, Val Loss: 4.2060, Val Accuracy: 0.4609, Val F1: 0.4480\n",
      "Class 0 - Precision: 0.8338, Recall: 0.3558, F1-Score: 0.4988\n",
      "Class 1 - Precision: 0.3081, Recall: 0.6522, F1-Score: 0.4185\n",
      "Class 2 - Precision: 0.3051, Recall: 0.6634, F1-Score: 0.4180\n",
      "Epoch 17/200 - Train Loss: 4.3865, Train Accuracy: 0.4609, Train F1: 0.4510, Val Loss: 3.9808, Val Accuracy: 0.4565, Val F1: 0.4451\n",
      "Class 0 - Precision: 0.8420, Recall: 0.3327, F1-Score: 0.4770\n",
      "Class 1 - Precision: 0.3055, Recall: 0.6645, F1-Score: 0.4186\n",
      "Class 2 - Precision: 0.3015, Recall: 0.6757, F1-Score: 0.4169\n",
      "Epoch 18/200 - Train Loss: 4.2968, Train Accuracy: 0.4610, Train F1: 0.4512, Val Loss: 4.1321, Val Accuracy: 0.4453, Val F1: 0.4375\n",
      "Class 0 - Precision: 0.8397, Recall: 0.3384, F1-Score: 0.4824\n",
      "Class 1 - Precision: 0.3066, Recall: 0.6668, F1-Score: 0.4201\n",
      "Class 2 - Precision: 0.3033, Recall: 0.6706, F1-Score: 0.4177\n",
      "Epoch 19/200 - Train Loss: 4.0695, Train Accuracy: 0.4616, Train F1: 0.4518, Val Loss: 3.9627, Val Accuracy: 0.4486, Val F1: 0.4401\n",
      "Class 0 - Precision: 0.8344, Recall: 0.3630, F1-Score: 0.5059\n",
      "Class 1 - Precision: 0.3097, Recall: 0.6558, F1-Score: 0.4207\n",
      "Class 2 - Precision: 0.3090, Recall: 0.6612, F1-Score: 0.4212\n",
      "Epoch 20/200 - Train Loss: 3.9829, Train Accuracy: 0.4631, Train F1: 0.4531, Val Loss: 3.5380, Val Accuracy: 0.4616, Val F1: 0.4493\n",
      "Class 0 - Precision: 0.8570, Recall: 0.3293, F1-Score: 0.4758\n",
      "Class 1 - Precision: 0.3095, Recall: 0.6954, F1-Score: 0.4283\n",
      "Class 2 - Precision: 0.3101, Recall: 0.6864, F1-Score: 0.4272\n",
      "Epoch 21/200 - Train Loss: 3.7106, Train Accuracy: 0.4640, Train F1: 0.4548, Val Loss: 3.7712, Val Accuracy: 0.4499, Val F1: 0.4438\n",
      "Class 0 - Precision: 0.8633, Recall: 0.3548, F1-Score: 0.5029\n",
      "Class 1 - Precision: 0.3189, Recall: 0.6918, F1-Score: 0.4366\n",
      "Class 2 - Precision: 0.3175, Recall: 0.6938, F1-Score: 0.4357\n",
      "Epoch 22/200 - Train Loss: 3.2598, Train Accuracy: 0.4731, Train F1: 0.4634, Val Loss: 2.7084, Val Accuracy: 0.4675, Val F1: 0.4584\n",
      "Model saved with Val F1: 0.4584\n",
      "Class 0 - Precision: 0.8629, Recall: 0.3626, F1-Score: 0.5106\n",
      "Class 1 - Precision: 0.3223, Recall: 0.6836, F1-Score: 0.4381\n",
      "Class 2 - Precision: 0.3194, Recall: 0.7015, F1-Score: 0.4389\n",
      "Epoch 23/200 - Train Loss: 2.7227, Train Accuracy: 0.4809, Train F1: 0.4702, Val Loss: 2.2889, Val Accuracy: 0.4726, Val F1: 0.4625\n",
      "Model saved with Val F1: 0.4625\n",
      "Class 0 - Precision: 0.8658, Recall: 0.3595, F1-Score: 0.5080\n",
      "Class 1 - Precision: 0.3205, Recall: 0.6948, F1-Score: 0.4387\n",
      "Class 2 - Precision: 0.3214, Recall: 0.6971, F1-Score: 0.4400\n",
      "Epoch 24/200 - Train Loss: 2.5607, Train Accuracy: 0.4845, Train F1: 0.4732, Val Loss: 2.2201, Val Accuracy: 0.4717, Val F1: 0.4622\n",
      "Class 0 - Precision: 0.8704, Recall: 0.3749, F1-Score: 0.5241\n",
      "Class 1 - Precision: 0.3391, Recall: 0.7113, F1-Score: 0.4593\n",
      "Class 2 - Precision: 0.3351, Recall: 0.7295, F1-Score: 0.4592\n",
      "Epoch 25/200 - Train Loss: 2.0948, Train Accuracy: 0.4903, Train F1: 0.4783, Val Loss: -0.0212, Val Accuracy: 0.4902, Val F1: 0.4809\n",
      "Model saved with Val F1: 0.4809\n",
      "Class 0 - Precision: 0.8860, Recall: 0.4013, F1-Score: 0.5524\n",
      "Class 1 - Precision: 0.3584, Recall: 0.7471, F1-Score: 0.4844\n",
      "Class 2 - Precision: 0.3571, Recall: 0.7504, F1-Score: 0.4839\n",
      "Epoch 26/200 - Train Loss: -0.9966, Train Accuracy: 0.5254, Train F1: 0.5096, Val Loss: -2.9460, Val Accuracy: 0.5172, Val F1: 0.5069\n",
      "Model saved with Val F1: 0.5069\n",
      "Class 0 - Precision: 0.8953, Recall: 0.4033, F1-Score: 0.5561\n",
      "Class 1 - Precision: 0.3664, Recall: 0.7646, F1-Score: 0.4954\n",
      "Class 2 - Precision: 0.3645, Recall: 0.7686, F1-Score: 0.4945\n",
      "Epoch 27/200 - Train Loss: -2.1893, Train Accuracy: 0.5424, Train F1: 0.5255, Val Loss: -4.0478, Val Accuracy: 0.5245, Val F1: 0.5153\n",
      "Model saved with Val F1: 0.5153\n",
      "Class 0 - Precision: 0.8975, Recall: 0.4290, F1-Score: 0.5805\n",
      "Class 1 - Precision: 0.3754, Recall: 0.7771, F1-Score: 0.5062\n",
      "Class 2 - Precision: 0.3783, Recall: 0.7627, F1-Score: 0.5058\n",
      "Epoch 28/200 - Train Loss: -2.9370, Train Accuracy: 0.5528, Train F1: 0.5351, Val Loss: -3.6908, Val Accuracy: 0.5427, Val F1: 0.5309\n",
      "Model saved with Val F1: 0.5309\n",
      "Class 0 - Precision: 0.9072, Recall: 0.4297, F1-Score: 0.5832\n",
      "Class 1 - Precision: 0.3858, Recall: 0.7908, F1-Score: 0.5186\n",
      "Class 2 - Precision: 0.3849, Recall: 0.7903, F1-Score: 0.5176\n",
      "Epoch 29/200 - Train Loss: -3.7072, Train Accuracy: 0.5619, Train F1: 0.5437, Val Loss: -6.0116, Val Accuracy: 0.5501, Val F1: 0.5398\n",
      "Model saved with Val F1: 0.5398\n",
      "Class 0 - Precision: 0.9154, Recall: 0.4632, F1-Score: 0.6152\n",
      "Class 1 - Precision: 0.4095, Recall: 0.8110, F1-Score: 0.5442\n",
      "Class 2 - Precision: 0.4074, Recall: 0.8120, F1-Score: 0.5426\n",
      "Epoch 30/200 - Train Loss: -5.1202, Train Accuracy: 0.5773, Train F1: 0.5586, Val Loss: -8.0099, Val Accuracy: 0.5794, Val F1: 0.5673\n",
      "Model saved with Val F1: 0.5673\n",
      "Class 0 - Precision: 0.9258, Recall: 0.4857, F1-Score: 0.6371\n",
      "Class 1 - Precision: 0.4257, Recall: 0.8267, F1-Score: 0.5620\n",
      "Class 2 - Precision: 0.4241, Recall: 0.8302, F1-Score: 0.5614\n",
      "Epoch 31/200 - Train Loss: -6.6812, Train Accuracy: 0.6021, Train F1: 0.5814, Val Loss: -9.4852, Val Accuracy: 0.6000, Val F1: 0.5868\n",
      "Model saved with Val F1: 0.5868\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn_struct(y_pred, y_batch)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Convert predictions to class labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_f1 = 0.0  # Track the best validation F1 score\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "val_losses, val_f1s, val_accuracies = [], [], []\n",
    "train_losses, train_f1s, train_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # For F1 score calculation\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch, encoded_structure=None)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn_struct(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        _, predicted_labels = torch.max(y_pred, dim=-1)\n",
    "\n",
    "        # Collect true and predicted labels for F1 calculation\n",
    "        all_true_labels.append(y_batch.cpu())\n",
    "        all_predicted_labels.append(predicted_labels.cpu())\n",
    "\n",
    "        # Calculate correct predictions\n",
    "        correct_preds += (predicted_labels == y_batch).sum().item()\n",
    "        total_preds += y_batch.numel()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Calculate F1 score for the training set\n",
    "    all_true_labels = torch.cat(all_true_labels)\n",
    "    all_predicted_labels = torch.cat(all_predicted_labels)\n",
    "    train_metrics = calculate_class_metrics(all_true_labels, all_predicted_labels)\n",
    "    avg_train_f1 = print_validation_metrics(train_metrics, printer=False)  # Passing 0.0 for validation loss as it's unused here\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    train_f1s.append(avg_train_f1)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        all_true_labels = []\n",
    "        all_predicted_labels = []\n",
    "\n",
    "        for X_val_batch, y_val_batch in val_dataloader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            y_val_pred = model(X_val_batch, encoded_structure=None)\n",
    "\n",
    "            # Calculate loss\n",
    "            val_loss += loss_fn_struct(y_val_pred, y_val_batch).item()\n",
    "\n",
    "            # Convert predictions to class labels\n",
    "            _, predicted_labels = torch.max(y_val_pred, dim=-1)\n",
    "\n",
    "            # Collect true and predicted labels for F1 calculation\n",
    "            all_true_labels.append(y_val_batch.cpu())\n",
    "            all_predicted_labels.append(predicted_labels.cpu())\n",
    "\n",
    "            # Calculate correct predictions\n",
    "            correct_preds += (predicted_labels == y_val_batch).sum().item()\n",
    "            total_preds += y_val_batch.numel()\n",
    "\n",
    "        # Calculate validation accuracy and loss\n",
    "        val_accuracy = correct_preds / total_preds\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        # Calculate F1 score for the validation set\n",
    "        all_true_labels = torch.cat(all_true_labels)\n",
    "        all_predicted_labels = torch.cat(all_predicted_labels)\n",
    "        val_metrics = calculate_class_metrics(all_true_labels, all_predicted_labels)\n",
    "        avg_val_f1 = print_validation_metrics(val_metrics)\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{200} - Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1: {avg_train_f1:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_f1s.append(avg_val_f1)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Save the model if the validation F1 score improves\n",
    "    if avg_val_f1 > best_val_f1:\n",
    "        best_val_f1 = avg_val_f1\n",
    "        torch.save(model.state_dict(), f\"12-2_pairbiasenc.pth\")\n",
    "        print(f\"Model saved with Val F1: {avg_val_f1:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c725209-9188-47c8-8f3a-3888deea5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to save each of the lists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Save lists to CSV files with filenames related to the model name\n",
    "model_name = \"12-2_pairbiasenc\"\n",
    "\n",
    "# Create a dictionary for each list to be saved\n",
    "data_to_save = {\n",
    "    f\"{model_name}_train_losses.csv\": train_losses,\n",
    "    f\"{model_name}_train_f1s.csv\": train_f1s,\n",
    "    f\"{model_name}_train_accuracies.csv\": train_accuracies,\n",
    "    f\"{model_name}_val_losses.csv\": val_losses,\n",
    "    f\"{model_name}_val_f1s.csv\": val_f1s,\n",
    "    f\"{model_name}_val_accuracies.csv\": val_accuracies,\n",
    "}\n",
    "\n",
    "# Loop through each list and save it as a CSV\n",
    "for filename, data_list in data_to_save.items():\n",
    "    # Convert list to DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=[filename.split('_')[-1].split('.')[0]])  # Use the metric name as column\n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "print(\"Metrics saved to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acaac97-25f1-4d85-8680-50d159aa215f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b67cb0-edd2-4887-958e-6475fd7a1030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e83bc70f-6404-4432-8519-f5b8ae844ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model on some sequences from the validation set:\n",
      "\n",
      "Sequence 1: CCCCAGTATAGCACCAAGGACAGTGCGTGTAACACTAGCAAGGTA\n",
      "True Structure:    ..........((((........))))(((...)))..........\n",
      "Predicted Structure: ..........((((........)))).((...))((....))...\n",
      "\n",
      "Sequence 2: GGTAAGAAGAGCCTAAAATCAGCGCGTAATAAGGGCAAAAGACCC\n",
      "True Structure:    ..........(((....................))).........\n",
      "Predicted Structure: ..........(((.....................))......)))\n",
      "\n",
      "Sequence 3: AGATAACTGGGTTTATTCTAAAAGGGACGACACAATAGGCTCGTC\n",
      "True Structure:    ....(((...)))............(((((..........)))))\n",
      "Predicted Structure: .(((......)))............(((((..........)))))\n",
      "\n",
      "Sequence 4: TTCCGATGGTTGTCCCGTCGCGACCTGTGCCCATTACTGACAGCC\n",
      "True Structure:    ...((((((.....))))))....((((...........))))..\n",
      "Predicted Structure: ...((((((.....))))))....((((.........).))))..\n",
      "\n",
      "Sequence 5: TGGTCTATCCTAGTTTAAGGTCCAGTTAAACGGGCGGTGACTTTC\n",
      "True Structure:    .((.....))..((((((.......))))))..............\n",
      "Predicted Structure: .((.....))..((((((.......))))))..............\n",
      "\n",
      "Sequence 6: CCGCGTTTAAAGTCGCCACATCCGCTCCGAGCACCATCTCGCTTT\n",
      "True Structure:    ..(((........)))...........((((......))))....\n",
      "Predicted Structure: ..(((........)))...........((((......))))....\n",
      "\n",
      "Sequence 7: CTAGAACAAACCGCTAAAAACCGATATAAAGGGCGACGAGAAATC\n",
      "True Structure:    ...........(((..................)))..((....))\n",
      "Predicted Structure: ...........((((................))))..((....))\n",
      "\n",
      "Sequence 8: TCCCTTTAGTACAAAAGCGCGGCACGGACTGACGTCCGTGAACAT\n",
      "True Structure:    ...((((......)))).....(((((((....))))))).....\n",
      "Predicted Structure: ...((((......)))).....(((((((....))))))).....\n",
      "\n",
      "Sequence 9: TGTAGAACTAAGTTCGTCATAAAGGCCGTTGTCAGATGAGATAAT\n",
      "True Structure:    ....((((...))))..(((...............))).......\n",
      "Predicted Structure: ....((((...))))..............................\n",
      "\n",
      "Sequence 10: TAGTCAGACGATTCGACCGAAGATTATCCGCTCTGAACGTGTGCT\n",
      "True Structure:    ....((.(((.(((((.((.........)).)).)))))).))..\n",
      "Predicted Structure: ....((.(((.(((((.............).))))))))).))..\n",
      "\n",
      "Sequence 11: TTACATATATAGGGTAATGTAGGCTGTACGAACGATCGAAGGAGC\n",
      "True Structure:    ..((((..........))))........(((....))).......\n",
      "Predicted Structure: ..((((..........))))........(((....))).......\n",
      "\n",
      "Sequence 12: CCATCGACATAAATTTAACCCCCGCGGTGCTAGACCCCACCCATC\n",
      "True Structure:    .........................((((........))))....\n",
      "Predicted Structure: .........................((((........))))....\n",
      "\n",
      "Sequence 13: ATGTCCAAGACAAAGATTGAAGAAGATCGGGCCAAAATCTCACAT\n",
      "True Structure:    .((((...)))).(((((.................))))).....\n",
      "Predicted Structure: .((((...))))....((.....(.((........))))).....\n",
      "\n",
      "Sequence 14: CTCAAGGAGTGATAGGACAGCGGCCCTAGCTGCGCCCACTAGGGG\n",
      "True Structure:    ..((.....))........(((((....))))).(((....))).\n",
      "Predicted Structure: (.((....)))........(((((....))))).(((....))).\n",
      "\n",
      "Sequence 15: GAGAACGATCGAGTGTACGTGAGAGCGCTGAACGGGGAGGCGCTC\n",
      "True Structure:    ....(((..........)))..((((((...........))))))\n",
      "Predicted Structure: .....((..(.......)))..((((((...........))))))\n",
      "\n",
      "Sequence 16: GCATCCGGCGCACCTAGCGTATCGAAATCGTGGCTCGGTCCAAGC\n",
      "True Structure:    ........(((.....)))...((....))..(((.......)))\n",
      "Predicted Structure: .....(..(((.....)))...((....))..(.........)))\n",
      "\n",
      "Sequence 17: GGCCGCCATTACCTGGATCTACGCAAATATCATTGTAAGAAACTA\n",
      "True Structure:    .....(((.....))).......(((......)))..........\n",
      "Predicted Structure: .....(((.....))).......(((......)))..........\n",
      "\n",
      "Sequence 18: TTGACCAAGTTCCTTCATAATACACGGGGAAAGCAGGCTTAGTAG\n",
      "True Structure:    .........((((..............))))..............\n",
      "Predicted Structure: .........((((........).....)))(((....))).....\n",
      "\n",
      "Sequence 19: CAATCATGTTCCGCTAGCAGATTTGTTCTTGCGACGGCAACCATA\n",
      "True Structure:    .......((((((...(((..........)))..))).)))....\n",
      "Predicted Structure: ...........(((.((((((.....))).))).).)..).....\n",
      "\n",
      "Sequence 20: AACGGTCACCAACCTGCTAGCTTTTCCGTCACAAGGTCTGCGGCG\n",
      "True Structure:    ...(((.....)))...........(((............)))..\n",
      "Predicted Structure: ...(((.....)))...........((((.........)))))..\n",
      "\n",
      "Sequence 21: ACGAGAGCCTGCAACGTCCCTCTACTCCTTGTTCTGTTATCTGGA\n",
      "True Structure:    ...((((............))))...((..............)).\n",
      "Predicted Structure: ...((((............))))((.....)).............\n",
      "\n",
      "Sequence 22: ACAATCCTATAGGGCCTTACAATAACATAGGCGACCCTTAGCCCA\n",
      "True Structure:    .....((....))................(((........)))..\n",
      "Predicted Structure: ......(...(((((((.............))..))))..)))).\n",
      "\n",
      "Sequence 23: ACCCTGTGTCGGAATGATCTACGCGGGAGCCATTGGGTAGTGTAG\n",
      "True Structure:    .(((.((((..((....)).)))))))..((....))........\n",
      "Predicted Structure: .((((((((((...)).)).))))))).)((....))........\n",
      "\n",
      "Sequence 24: AAGTCGCTATGGCTGGGTATAATTCCGTTCGGCTGATGCATTTGC\n",
      "True Structure:    .....((....))..................((....))......\n",
      "Predicted Structure: .....((....))...........((....)).....((....))\n",
      "\n",
      "Sequence 25: GGATGCGGCCAGCGCCCGGTCGTATATGGTTAAATAACATCGCCT\n",
      "True Structure:    ....(((.....))).............(((....))).......\n",
      "Predicted Structure: ....((((....)))).)))).......(((....))).......\n",
      "\n",
      "Sequence 26: AATTTTGCGTTAGGGGTACTATCTCCTAGGGAAGCTATCGTCAAG\n",
      "True Structure:    ...........(((((......)))))..................\n",
      "Predicted Structure: ......((..((((((......)))))).....)))..)......\n",
      "\n",
      "Sequence 27: AGACGATTCTTTAGGATGGTGCAGTCACTTATCTCGGCAGCTCAC\n",
      "True Structure:    ..................(((....))).................\n",
      "Predicted Structure: (((....)))....(((.(((....)))..)))............\n",
      "\n",
      "Sequence 28: AAGTTCCGTCAAACCCGCAGTTCTTTAGCCGACCAGTCCCTTCTA\n",
      "True Structure:    ...........(((.....)))........((....)).......\n",
      "Predicted Structure: ..(((......)))......))........((....)).......\n",
      "\n",
      "Sequence 29: TATACTGAGTCTTTGTGTGTTTCACTGGCTTTCCGGGAAGTGGTT\n",
      "True Structure:    ..............(((.....)))...((((....)))).....\n",
      "Predicted Structure: ..............(((.(...)))...(((.((...))))))..\n",
      "\n",
      "Sequence 30: GGGAGGCCAAGGGGAGGACGGAAACATATCCCAAGCGGACAAGTA\n",
      "True Structure:    ((....))...((((.............)))).............\n",
      "Predicted Structure: ((....))...((((.............)))).....).......\n",
      "\n",
      "Sequence 31: GGAGACCACCCGTGATTGTCCTCGAGGTCTACATGGTCGACATGC\n",
      "True Structure:    ..(((((...((..........)).))))).((((.....)))).\n",
      "Predicted Structure: ..(((((...((..........)).))))).((((.....)))).\n",
      "\n",
      "Sequence 32: AATAAGCGTCTTCTGTCTTAGCAGCGCGTACCTTCGTTGACGGTG\n",
      "True Structure:    ......((((..((((....))))..((......))..))))...\n",
      "Predicted Structure: .....((((...((((....))))))).)(((.((....))))).\n",
      "\n",
      "Sequence 33: ATGTAATATGGTTATCACTCTTGTGATCTTAGATTCTCGAGTCAG\n",
      "True Structure:    .............(((((....)))))..................\n",
      "Predicted Structure: .............(((((....)))))....(((......)))..\n",
      "\n",
      "Sequence 34: CAGGTGGCCAACAGATCCAAGGCGCATCCCCCATGCACTAGAAAT\n",
      "True Structure:    ......(((...........)))((((.....)))).........\n",
      "Predicted Structure: ...(..(((.......))..)))((((.....)))).........\n",
      "\n",
      "Sequence 35: AATAAGAGGATCCGTGTACGTTGATACTGGGAAGGGGGCGACAGC\n",
      "True Structure:    ............((....))......((....))...((....))\n",
      "Predicted Structure: ............((....))......((....))...((....))\n",
      "\n",
      "Sequence 36: CTTAAGTTGTTGGCTACGGGTGACGCCATTGAGGGGATTACGAAA\n",
      "True Structure:    ...........(((..........)))..................\n",
      "Predicted Structure: .............(..((......)))..................\n",
      "\n",
      "Sequence 37: CATCGAATATTTAGAATCCCCGTACGTGGGATGCCATGCGCAACA\n",
      "True Structure:    ..................((((....))))..((.....))....\n",
      "Predicted Structure: .................(((((....)))))(((.....)))...\n",
      "\n",
      "Sequence 38: ACGGGGCTCCCTCGCGTGCTCTTCGTTTTATCGGATGGCAGAGCT\n",
      "True Structure:    .(((((...)))))...(((((.................))))).\n",
      "Predicted Structure: .((((...)))))))...((((.((......).......))))).\n",
      "\n",
      "Sequence 39: CTAGCTAGGCGTGTGCTTAAACAGGCCCTTCCTATGGATTCACCT\n",
      "True Structure:    .......(((.(((......))).)))........((.....)).\n",
      "Predicted Structure: .......(((.(((......))).)))........((.....)).\n",
      "\n",
      "Sequence 40: CGGTCGCTAACTCGCCAGTGTCATGACCCTACGCCACACCTACAA\n",
      "True Structure:    .((((...................)))).................\n",
      "Predicted Structure: .(((((...))..)))))((....)))).......))))......\n",
      "\n",
      "Sequence 41: CGGCTATTCTTACGTTTGCGAATTTAGAAGGAATCGGTTCAACTT\n",
      "True Structure:    ............((....))..........(((....))).....\n",
      "Predicted Structure: ............((....))..........(((....))).....\n",
      "\n",
      "Sequence 42: CCGGACCGATAAGTCATTTAAGCCTAAGAGAGACGTCTGTACAAA\n",
      "True Structure:    ...(((......))).............(((....))).......\n",
      "Predicted Structure: ...(((......))).............(((....))).......\n",
      "\n",
      "Sequence 43: CGTCTGTCCTATTTCTCGGAACTAAGGATATATGAAACGGTGCTA\n",
      "True Structure:    .......(((..............)))..................\n",
      "Predicted Structure: ........((.(((.....))...)))..................\n",
      "\n",
      "Sequence 44: CCAGGTCCTTAGGGACGCGATTAGCGAGAGAAGATCGTCCGCCCT\n",
      "True Structure:    ....((((....)))).((((............))))........\n",
      "Predicted Structure: .....(((.....)))..(((..((((......))))..)))...\n",
      "\n",
      "Sequence 45: CGTCAACGACCTTGTCGCCACGAAACACGTTAGAATTGGGGTCGA\n",
      "True Structure:    ......(((((........(((.....))).........))))).\n",
      "Predicted Structure: .(....((((....)))..(((.....))).........))))).\n",
      "\n",
      "Sequence 46: TTGATTTTTAAGGCACTTATCTGTTCGCTAGCGGCCATATGACTT\n",
      "True Structure:    .........(((...))).......((....))............\n",
      "Predicted Structure: .........(((...))).......(((...)))...........\n",
      "\n",
      "Sequence 47: ATCCGCTCCTTTCCTGCGGGGGAATCACGCAGCCCTTGCCAACGT\n",
      "True Structure:    ..((((.........)))).........(((.....)))......\n",
      "Predicted Structure: ..((((.........)))).........((((....))).....)\n",
      "\n",
      "Sequence 48: CCTGGCGTCAAGTACGCGGACGGATGGATCGAAATCGTGGCTCCC\n",
      "True Structure:    ....((((.....))))..(((.............))).......\n",
      "Predicted Structure: ....((((.....))))............((....))........\n",
      "\n",
      "Sequence 49: AGAGAGATTTCACGCGGCCACGATAGCTCAGCGAGGTTAGAAATC\n",
      "True Structure:    .....((((((...((....))....(((...)))....))))))\n",
      "Predicted Structure: .......((((.((((((.......))....))......))))))\n",
      "\n",
      "Sequence 50: TACTGAGACCGTATGAACATCAACCGCGATGGGAGTAGCACTATG\n",
      "True Structure:    .................((((......))))..(((...)))...\n",
      "Predicted Structure: .................((((......))))..(((...)))...\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model_path = \"12-2_pairbiasenc.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Function to decode structure\n",
    "def decode_structure(encoded):\n",
    "    structure_mapping = {0: '.', 1: '(', 2: ')', 3: '-'}  # '-' represents padding\n",
    "    return ''.join([structure_mapping[code.item()] for code in encoded])\n",
    "\n",
    "def enforce_symmetry_and_minimum_distance(pred_structure, min_distance=3):\n",
    "    stack = []\n",
    "    for i, char in enumerate(pred_structure):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                opening_index = stack.pop()\n",
    "                # Enforce minimum distance rule\n",
    "                if i - opening_index - 1 < min_distance or ''.join(pred_structure[opening_index + 1:i]).count('.') < min_distance:\n",
    "                    # Replace invalid pair with dots\n",
    "                    pred_structure[opening_index] = '.'\n",
    "                    pred_structure[i] = '.'\n",
    "    # Replace unmatched '(' with '.'\n",
    "    for i in stack:\n",
    "        pred_structure[i] = '.'\n",
    "    return pred_structure\n",
    "\n",
    "\n",
    "\n",
    "# Pick some sequences from the validation set\n",
    "num_test_sequences = 50  # Number of sequences to test\n",
    "val_sequences = X_val[:num_test_sequences]\n",
    "val_structures = y_struct_val[:num_test_sequences]\n",
    "\n",
    "# Run predictions and compare with ground truth\n",
    "print(\"Testing the model on some sequences from the validation set:\")\n",
    "with torch.no_grad():\n",
    "    for i, (sequence, true_structure) in enumerate(zip(val_sequences, val_structures)):\n",
    "        sequence = sequence.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        true_structure = true_structure.to(device)\n",
    "        \n",
    "        # Predict structure\n",
    "        pred_structure_logits = model(sequence, true_structure.unsqueeze(0))\n",
    "        pred_structure = torch.argmax(pred_structure_logits.view(-1, 3), dim=-1)\n",
    "        \n",
    "        # Decode sequence and structures\n",
    "        decoded_sequence = ''.join([list('ATCG-')[x.item()] for x in sequence[0]])  # Decode sequence\n",
    "        decoded_true_structure = decode_structure(true_structure)\n",
    "        decoded_pred_structure = decode_structure(pred_structure)\n",
    "\n",
    "        # Enforce symmetry and minimum distance rule\n",
    "        decoded_pred_structure = enforce_symmetry_and_minimum_distance(list(decoded_pred_structure))\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nSequence {i + 1}: {decoded_sequence}\")\n",
    "        print(f\"True Structure:    {decoded_true_structure}\")\n",
    "        print(f\"Predicted Structure: {''.join(decoded_pred_structure)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad42b62-2112-4dbc-babf-7ad0ce1a71bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd878a-7427-4e67-ac28-73ca3c8d160b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
